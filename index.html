<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>ICCV 2025 Workshop on Knowledge-Intensive Multimodal Reasoning</title>
  <meta content="ICCV 2025 Workshop on Knowledge-Intensive Multimodal Reasoning" name="description">
  <meta content="ICCV 2025, workshop, multimodal, reasoning, knowledge-intensive" name="keywords">

  <!-- Favicons -->
  <link href="assets/img/favicon.png" rel="icon">
  <link href="assets/img/apple-touch-icon.png" rel="apple-touch-icon">

  <!-- Google Fonts: preconnect + swap to avoid render blocking -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@300;400;700&family=Raleway:wght@400;500;700;800&display=swap" rel="stylesheet">

  <!-- Critical CSS (keep render-blocking minimal) -->
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/css/style.css" rel="stylesheet">

  <!-- Non-critical vendor CSS: load without blocking -->
  <link rel="preload" href="assets/vendor/bootstrap-icons/bootstrap-icons.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <noscript><link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet"></noscript>

  <link rel="preload" href="assets/vendor/aos/aos.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <noscript><link href="assets/vendor/aos/aos.css" rel="stylesheet"></noscript>

  <link rel="preload" href="assets/vendor/glightbox/css/glightbox.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <noscript><link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet"></noscript>

  <link rel="preload" href="assets/vendor/swiper/swiper-bundle.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <noscript><link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet"></noscript>

  <!-- Custom CSS for Schedule Table and performance -->
  <style>
    .schedule-table {
      width: 100%;
      border-collapse: collapse;
      background: #fff;
      font-family: Arial, sans-serif;
      box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
      margin: 20px 0;
    }
    .schedule-table th,
    .schedule-table td {
      padding: 12px 15px;
      border-bottom: 1px solid #ccc;
      text-align: left;
    }
    .schedule-table th {
      background-color: #f7f7f7;
      font-weight: bold;
    }
    .schedule-table tbody tr:nth-child(even) {
      background-color: #f9f9f9;
    }
    .schedule-table tbody tr:hover {
      background-color: #f1f1f1;
    }
    /* Responsive table paddings */
    @media (max-width: 768px) {
      .schedule-table th,
      .schedule-table td {
        padding: 10px;
      }
    }
    /* Keep time on a single line */
    .schedule-table td.time,
    .schedule-table th:first-child {
      white-space: nowrap;
    }
    /* Distinct background colors for special rows */
    .schedule-table tr.break-row td   { background-color: #fff3cd80 !important; }
    .schedule-table tr.poster-row td  { background-color: #e7f5ff80 !important; }
    /* Grey variant for specific short breaks */
    .schedule-table tr.gray-break td  { background-color: #f8f9fa !important; }

    /* Four speakers per row on larger screens (yields 4 + 3 for 7 profiles) */
    @media (min-width: 768px) {
      #speakers .speaker-col-5 {
        flex: 0 0 25%;
        max-width: 25%;
      }
    }

    /* Performance: avoid work for offscreen sections until scrolled into view */
    #speakers, #organizers, #sponsors, #grant, #accepted-paper, #topic, #cfp {
      content-visibility: auto;
      contain-intrinsic-size: 1000px 1px;
    }
  </style>
  <style>
    /* Make topic toggles feel clickable and add explicit icon */
    #topic summary {
      cursor: pointer;
      list-style: none;
      position: relative;
      padding-left: 1.2em;
    }
    #topic summary::-webkit-details-marker { display: none; }
    #topic details > summary::before {
      content: '▶';
      position: absolute;
      left: 0;
      top: 0;
    }
    #topic details[open] > summary::before {
      content: '▼';
    }
    #topic details > summary b { display: inline-block; }
  </style>

  <!-- =======================================================
  * Template Name: TheEvent
  * Updated: Sep 18 2023 with Bootstrap v5.3.2
  * Template URL: https://bootstrapmade.com/theevent-conference-event-bootstrap-template/
  * Author: BootstrapMade.com
  * License: https://bootstrapmade.com/license/
  ======================================================== -->
</head>

<body>

  <!-- ======= Header ======= -->
  <header id="header" class="d-flex align-items-center ">
    <div class="container-fluid container-xxl d-flex align-items-center">

      <div id="logo" class="me-auto">
        <!-- Optional text logo -->
        <!-- <h1><a href="index.html">The<span>Event</span></a></h1> -->
        <a href="index.html" class="scrollto">
          <img src="assets/img/logo.svg" alt="Workshop Logo" title="Knowledge-Intensive Multimodal Reasoning"
               width="160" height="40" style="height:auto" decoding="async">
        </a>
      </div>

      <nav id="navbar" class="navbar order-last order-lg-0">
        <ul>
          <li><a class="nav-link scrollto active" href="#hero">Home</a></li>
          <li><a class="nav-link scrollto" href="#about">About</a></li>
          <li><a class="nav-link scrollto" href="#schedule">Schedule</a></li>
          <li><a class="nav-link scrollto" href="#topic">Topics</a></li>
          <li><a class="nav-link scrollto" href="#cfp">Call for Papers</a></li>
          <li><a class="nav-link scrollto" href="#accepted-paper">Accepted Papers</a></li>
          <li><a class="nav-link scrollto" href="#grant">Student Registration Grant</a></li>
          <li><a class="nav-link scrollto" href="#speakers">Speakers</a></li>
          <li><a class="nav-link scrollto" href="#organizers">Organizers</a></li>
        </ul>
        <i class="bi bi-list mobile-nav-toggle"></i>
      </nav>

    </div>
  </header><!-- End Header -->

  <!-- ======= Hero Section ======= -->
  <section id="hero">
    <div class="hero-container" data-aos="zoom-in" data-aos-delay="200">
      <h1 class="mb-4 pb-0" style="text-transform: none;">
        Knowledge-Intensive Multimodal Reasoning<br>
        ICCV 2025, October 19 2025, Hawaii
      </h1>
      <!--
      <div class="social-links" style="margin-top: 25px;">
        <a href="https://x.com/ZhiyuanCS/status/1866401974792691742" target="_blank" rel="noopener noreferrer" style="margin-right: 15px; text-decoration: none; color: #1DA1F2;">
          Twitter
        </a>
        <a href="https://openreview.net/group?id=ICLR.cc/2025/Workshop/LLM_Reason_and_Plan&referrer=%5BHomepage%5D(%2F)#tab-your-consoles" target="_blank" rel="noopener noreferrer" style="text-decoration: none; color: #1DA1F2;">
          OpenReview
        </a>
      </div>
      -->
    </div>
  </section>
  <!-- End Hero Section -->

  <main id="main">

    <!-- ======= About Section ======= -->
    <section id="about">
      <div class="container position-relative" data-aos="fade-up">
        <div class="row">
          <div class="col-lg-12">
            <h2>About The Workshop</h2>
            <p style="font-size: 18px;">This workshop aims to advance the frontier of multimodal AI systems that can effectively reason across specialized domains requiring extensive domain knowledge. Recent advancements in multimodal AI—combining information from text, images, audio, and structured data—have unlocked impressive capabilities in general-purpose reasoning. However, significant challenges persist when these systems encounter scenarios demanding deep domain expertise in fields such as medicine, engineering, and scientific research. Such contexts require expert-level perception and reasoning grounded in extensive subject knowledge, highlighting the need for specialized strategies to handle domain-specific complexity. Through invited talks, panel discussions, and interactive poster sessions, researchers and practitioners from diverse backgrounds will share the latest developments, ongoing hurdles, and promising future directions for knowledge-intensive multimodal reasoning. The workshop aims to foster collaboration and stimulate innovation towards the development of next-generation multimodal AI systems capable of reliable, transparent, and contextually grounded reasoning in specialized, high-stakes environments.</p>
          </div>
        </div>
      </div>
    </section>
    <!-- End About Section -->

    <!-- ======= Schedule Section ======= -->
    <section id="schedule" style="padding-top: 20px;">
      <div class="container" data-aos="fade-up">
        <div class="section-header">
          <h2>Schedule</h2>
        </div>

        <div class="table-responsive">
          <table class="schedule-table table align-middle">
            <thead>
              <tr>
                <th>Time&nbsp;(HST)</th>
                <th>Session</th>
                <th>Speaker</th>
                <th>Talk&nbsp;Title</th>
              </tr>
            </thead>
            <tbody>
              <tr class="opening-row">
                <td class="time">1:00 – 1:10</td>
                <td colspan="3">Opening Remarks</td>
              </tr>

              <tr>
                <td class="time">1:10 – 1:40</td>
                <td>Invited Talk&nbsp;1</td>
                <td>Yongming Rao (Tencent Hunyuan)</td>
                <td>Interactive Multi-Modal Reasoning with Thinking-on-Image Reasoning</td>
              </tr>
              <tr>
                <td class="time">1:40 – 2:10</td>
                <td>Invited Talk&nbsp;2</td>
                <td>Kang-Fu Mei (Google DeepMind)</td>
                <td>The Power of Context: How Multimodality Reasoning Improves Image Generation</td>
              </tr>
              
              <tr>
                <td class="time">2:10 – 2:20</td>
                <td>Oral Paper 1</td>
                <td>Enxin Song (UCSD &amp; ZJU)</td>
                <td>Video-MMLU: A Massive Multi-Discipline Lecture Understanding Benchmark</td>
              </tr>
              
              <tr class="poster-row">
                <td class="time">2:20 – 3:20</td>
                <td colspan="3">Poster Session</td>
              </tr>
              <tr>
                <td class="time">3:20 – 3:30</td>
                <td>Oral Paper 2</td>
                <td>Ziqi Huang (NTU)</td>
                <td>VChain: Chain-of-Visual-Thought for Reasoning in Video Generation</td>
              </tr>
              <tr>
                <td class="time">3:30 – 4:00</td>
                <td>Invited Talk&nbsp;3</td>
                <td>Ziwei Liu (NTU)</td>
                <td>Native Multimodal Models: Architecture, Post-Training, and Evaluation</td>
              </tr>
              <tr>
                <td class="time">4:00 – 4:30</td>
                <td>Invited Talk&nbsp;4</td>
                <td>David Fan (Meta FAIR)</td>
                <td></td>
              </tr>
              <tr class="break-row gray-break">
                <td class="time">4:30 – 4:40</td>
                <td colspan="3">Short break</td>
              </tr>
              <tr>
                <td class="time">4:40 – 4:50</td>
                <td>Oral Paper 3</td>
                <td>Shoubin Yu (UNC)</td>
                <td>SciVideoBench: Benchmarking Scientific Video Reasoning in Large Multimodal Models</td>
              </tr>
              <tr>
                <td class="time">4:50 – 5:00</td>
                <td>Oral Paper 4</td>
                <td>Zeyuan Yang (UMass)</td>
                <td>Machine Mental Imagery: Empower Multimodal Reasoning with Latent Visual Tokens</td>
              </tr>
              <tr>
                <td class="time">5:00 – 5:30</td>
                <td>Panel Session</td>
                <td></td>
                <td></td>
              </tr>
              <tr class="closing-row">
                <td class="time">5:30 – 5:40</td>
                <td colspan="3">Paper Award &amp; Closing Remarks</td>
              </tr>
            </tbody>
          </table>
        </div>
      </div>
    </section>
    <!-- End Schedule Section -->

    

    

    <!-- ======= Speakers ======= -->
    <section id="speakers" class="section-with-bg">
      <div class="container" data-aos="fade-up">
        <div class="section-header">
          <h2>Speakers and Panelists</h2>
        </div>
        <div class="row">
          

          <div class="speaker-col-5 col-sm-4 col-6">
            <div class="speaker" data-aos="fade-up" data-aos-delay="200" style="text-align: center;">
              <img src="assets/img/organizers/speaker/ziwei.png" alt="Ziwei Liu" class="img-fluid"
                   loading="lazy" decoding="async" width="320" height="320" style="height:auto">
              <br><a href="https://liuziwei7.github.io/" style="font-size: 1.2em; color: #00356B;">Ziwei Liu</a></br>
              <p>NTU</p>
            </div>
          </div>

          <div class="speaker-col-5 col-sm-4 col-6">
            <div class="speaker" data-aos="fade-up" data-aos-delay="200" style="text-align: center;">
              <img src="assets/img/organizers/speaker/Kangfu_DR.jpg" alt="Kang-Fu Mei" class="img-fluid"
                   loading="lazy" decoding="async" width="320" height="320" style="height:auto">
              <br><a href="https://kfmei.com/" style="font-size: 1.2em; color: #00356B;">Kang-Fu Mei</a></br>
              <p>Google DeepMind</p>
            </div>
          </div>

          <div class="speaker-col-5 col-sm-4 col-6">
            <div class="speaker" data-aos="fade-up" data-aos-delay="200" style="text-align: center;">
              <img src="assets/img/organizers/speaker/david.jpg" alt="David Fan" class="img-fluid"
                   loading="lazy" decoding="async" width="320" height="320" style="height:auto">
              <br><a href="https://davidfan.io/" style="font-size: 1.2em; color: #00356B;">David Fan</a></br>
              <p>Meta FAIR</p>
            </div>
          </div>

          <div class="speaker-col-5 col-sm-4 col-6">
            <div class="speaker" data-aos="fade-up" data-aos-delay="200" style="text-align: center;">
              <img src="assets/img/organizers/speaker/yongming.png" alt="Yongming Rao" class="img-fluid"
                   loading="lazy" decoding="async" width="320" height="320" style="height:auto">
              <br><a href="https://raoyongming.github.io/" style="font-size: 1.2em; color: #00356B;">Yongming Rao</a></br>
              <p>Tencent Hunyuan</p>
            </div>
          </div>

          <div class="speaker-col-5 col-sm-4 col-6">
            <div class="speaker" data-aos="fade-up" data-aos-delay="200" style="text-align: center;">
              <img src="assets/img/organizers/speaker/ranjay.jpeg" alt="Ranjay Krishna" class="img-fluid"
                   loading="lazy" decoding="async" width="320" height="320" style="height:auto">
              <br><a href="https://www.ranjaykrishna.com/" style="font-size: 1.2em; color: #00356B;">Ranjay Krishna</a></br>
              <p>UW & AI2</p>
            </div>
          </div>

          <div class="speaker-col-5 col-sm-4 col-6">
            <div class="speaker" data-aos="fade-up" data-aos-delay="200" style="text-align: center;">
              <img src="assets/img/organizers/faculty/manling.jpg" alt="Manling Li" class="img-fluid"
                   loading="lazy" decoding="async" width="320" height="320" style="height:auto">
              <br><a href="https://limanling.github.io/" style="font-size: 1.2em; color: #00356B;">Manling Li</a></br>
              <p>Northwestern</p>
            </div>
          </div>

          <div class="speaker-col-5 col-sm-4 col-6">
            <div class="speaker" data-aos="fade-up" data-aos-delay="200" style="text-align: center;">
              <img src="assets/img/organizers/faculty/arman.jpg" alt="Arman Cohan" class="img-fluid"
                   loading="lazy" decoding="async" width="320" height="320" style="height:auto">
              <br><a href="https://armancohan.com/" style="font-size: 1.2em; color: #00356B;">Arman Cohan</a></br>
              <p>Yale</p>
            </div>
          </div>

          <div class="speaker-col-5 col-sm-4 col-6">
            <div class="speaker" data-aos="fade-up" data-aos-delay="200" style="text-align: center;">
              <img src="assets/img/organizers/speaker/eric.jpeg" alt="Xin (Eric) Wang" class="img-fluid"
                   loading="lazy" decoding="async" width="320" height="320" style="height:auto">
              <br><a href="https://eric-xw.github.io/" style="font-size: 1.2em; color: #00356B;">Xin (Eric) Wang</a></br>
              <p>UCSB</p>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End Speakers -->

    <!-- ======= Topics Section ======= -->
    <section id="topic" class="section-with-bg">
      <br>
      <br>
      <div class="container" data-aos="fade-up">
        <div class="section-header">
          <h2>Topics</h2>
          <p>The workshop will cover a range of topics, including but not limited to:</p>
        </div>
        <section id="topics">
          <div class="container">
            <div class="topic-panel interactive-panel p-4 border shadow-sm rounded" style="background: #ffffff;">
              <details>
                <summary><b style="font-size: 18px; color: #343a40;">Knowledge-intensive Multimodal Learning</b></summary>
                <p style="color: #6c757d; margin-top: 10px;">
                  This topic focuses on method and architecture designs that integrate domain-specific knowledge with diverse data sources (e.g., text, images, sensor data, and structured data) across specialized fields. We will cover data curation strategies, modality fusion techniques, representation learning frameworks, and explainability methods aimed at ensuring that models capture the domain knowledge crucial for reliable reasoning in high-stakes settings.
                </p>
              </details>

              <hr>

              <details>
                <summary><b style="font-size: 18px; color: #343a40;">Multimodal Foundation Models for Specialized Domains</b></summary>
                <p style="color: #6c757d; margin-top: 10px;">
                  This topic investigates how to adapt large-scale and general-purpose multimodal foundation models for domains where specialized expertise is essential, such as clinical diagnostics, scientific research, and advanced engineering applications. We will cover strategies for efficient fine-tuning, prompt engineering, domain-centric pre-training, and knowledge distillation to blend foundational capabilities with expert-level insights.
                </p>
              </details>

              <hr>

              <details>
                <summary><b style="font-size: 18px; color: #343a40;">Embodied AI for Knowledge-Intensive Scenarios</b></summary>
                <p style="color: #6c757d; margin-top: 10px;">
                  This topic explores the integration of multimodal reasoning in physical or interactive domains, ranging from industrial automation to laboratory robotics. Key discussion points include sensor fusion, adaptive learning with minimal supervision, human-robot collaboration, and simulation-to-real transfer in safety-critical scenarios. Emphasis will be placed on how advanced reasoning techniques—grounded in specialized domain knowledge—can help ensure transparency, robustness, and trustworthiness in embodied AI systems.
                </p>
              </details>

              <hr>

              <details>
                <summary><b style="font-size: 18px; color: #343a40;">Evaluation and Benchmarking</b></summary>
                <p style="color: #6c757d; margin-top: 10px;">
                  Robust evaluation protocols and benchmarks are essential for gauging progress and ensuring the reliability of domain-specific multimodal AI. We will cover the development of standardized benchmarks, performance metrics, and testing methodologies designed to capture the full spectrum of specialized domain challenges for multimodal reasoning.
                </p>
              </details>

              <hr>

              <details>
                <summary><b style="font-size: 18px; color: #343a40;">Broader Topics in Knowledge-Intensive Multimodal Reasoning</b></summary>
                <p style="color: #6c757d; margin-top: 10px;">
                  In addition to the core themes above, our discussions will expand to emerging areas such as integrating symbolic and neural methods for structured reasoning, ensuring privacy and security with sensitive data, exploring multi-agent collaboration for complex decision-making, and examining societal and ethical considerations when deploying multimodal systems in real-world, high-stakes environments.
                </p>
              </details>
            </div>
          </div>
        </section>
      </div>
      <br>
      <br>
    </section>
    <!-- End Topics Section -->

    <!-- ======= Accepted Papers ======= -->
    <section id="accepted-paper" class="section-with-bg">
      <div class="container" data-aos="fade-up">
        <div class="section-header">
          <h2>Accepted Papers</h2>
        </div>
        <div id="poster-presentations">
          <ul id="posterList"></ul>
        </div>
      </div>
    </section>
    <!-- End Accepted Papers -->

    <!-- ======= Call For Papers ======= -->
    <section id="cfp">
      <div class="container" data-aos="fade-up">
        <div class="section-header">
          <h2>Call For Papers</h2>
        </div>
        <h3>Key Dates</h3>
        <ul class="fa-ul">
          <li>
            <b>Submission Deadline</b>:
            <s>August 5, 2025</s>
            <b style="color: red;">August 22, 2025</b> (AOE) - Extended to facilitate EMNLP submission papers
          </li>
          <li>
            <b>Notification</b>:
            <s>August 25, 2025 (AOE)</s>
            <b style="color: red;">September 2, 2025</b>
          </li>
        </ul>
        Deadlines are strict and will not be extended under any circumstances. All deadlines follow the
        <a href="https://time.is/Anywhere_on_Earth" target="_blank" rel="noopener noreferrer">Anywhere on Earth (AoE)</a> timezone.
        <br><br>
        <h3>Submission Site</h3>
        <p>
          Submissions are managed via
          <a href="https://openreview.net/group?id=thecvf.com/ICCV/2025/Workshop/KnowledgeMR" target="_blank" rel="noopener noreferrer">OpenReview</a>.
          Papers will remain private during the review process. All authors must maintain up-to-date OpenReview profiles to ensure proper conflict-of-interest management and paper matching. Incomplete profiles may result in desk rejection.
        </p>
        <h3>Submission Format</h3>
        <p>
          Papers are limited to eight pages, including figures and tables, in the
          <a href="https://www.overleaf.com/read/sjfsgkwvnjsq#8d8447" target="_blank" rel="noopener noreferrer">KnowledgeMR Workshop Latex Template</a>
          (adopted from the ICCV 2025 template). Additional pages containing cited references and appendix are allowed. Papers that are not properly anonymized, or do not use the template, or have more than eight pages (excluding references and appendix) will be rejected without review.
        </p>
        <h3>Anonymity</h3>
        <p>
          Double blind review: Our reviewing is double blind, in that authors do not know the names of the area chairs or reviewers for their papers, and the area chairs/reviewers cannot, beyond a reasonable doubt, infer the names of the authors from the submission and the additional material.
        </p>
        <h3>Dual Submission and Non-Archival Policy</h3>
        <p>
          Submissions under review at other venues will be accepted, provided they do not breach any dual-submission or anonymity policies of those venues. Submissions will not be indexed or have archival proceedings. We welcome ICCV 25, EMNLP 25, ICLR 26, AAAI 26 submissions.
        </p>
      </div>
    </section>
    <!-- End CFP -->

    

    <!-- ======= Student Registration Grant ======= -->
    <br><br>
    <section id="grant">
      <div class="container" data-aos="fade-up">
        <div class="section-header">
          <h2>Student Registration Grant</h2>
        </div>
        <p>
          We are excited to offer a limited number of free full conference, "student early" registrations for ICCV 2025, exclusively for full-time students attending in person. This initiative aims to support early-career researchers while fostering diversity, equity, and inclusion (DEI) in the academic community.
        </p>

        <h3>Selection Criteria</h3>
        <p>
          Applications will be evaluated based on the strength of the submitted materials (see details below). Priority will be given to students presenting papers at our workshop who lack alternative travel support.
        </p>
        <h3>How to Apply</h3>
        <p>
          Interested students must complete the application form
          <a href="https://forms.gle/PjKXLnFCtigt9LQa9" target="_blank" rel="noopener noreferrer">here</a>
          by <b>11:59pm (AoE) on August 26, 2025</b>, which includes the following:
        </p>
        <ul>
          <li><b>Personal &amp; Academic Details</b>: Name, affiliation, and relevant academic information</li>
          <li><b>CV/Resume</b></li>
          <li><b>Paper ID</b>: Accepted or submitted to our workshop</li>
          <li><b>Statement of Interest</b>: A brief paragraph explaining how this opportunity will benefit your research and career</li>
          <li><b>Attendance Confirmation</b>: A clear statement confirming that you will attend in person</li>
        </ul>
        <h3>Important Notes</h3>
        <ul>
          <li>Awardees will be announced on September 3, 2025</li>
          <li>If you have already registered, please submit your receipt, and we will provide further instructions</li>
          <li>Travel and accommodations must be arranged independently; this grant covers registration only</li>
        </ul>
        <p>
          This opportunity is highly competitive, and we encourage all eligible students to apply early.
        </p>
      </div>
    </section>
    <!-- End Grant -->

    <!-- ======= Organizers ======= -->
    <section id="organizers">
      <div class="container" data-aos="fade-up">
        <div class="section-header">
          <h2>Organizers</h2>
          <p>This workshop is organized by</p>
        </div>

        <div class="row">
          <div class="col-lg-2 col-md-3 col-sm-4 col-6">
            <div class="speaker" data-aos="fade-up" data-aos-delay="200" style="text-align: center;">
              <img src="assets/img/organizers/faculty/arman.jpg" alt="Arman Cohan" class="img-fluid"
                   loading="lazy" decoding="async" width="320" height="320" style="height:auto">
              <br><a href="https://armancohan.com/" style="font-size: 1.2em; color: #00356B;">Arman Cohan</a></br>
              <p>Yale</p>
            </div>
          </div>

          <div class="col-lg-2 col-md-3 col-sm-4 col-6">
            <div class="speaker" data-aos="fade-up" data-aos-delay="200" style="text-align: center;">
              <img src="assets/img/organizers/faculty/xiangliang.png" alt="Xiangliang Zhang" class="img-fluid"
                   loading="lazy" decoding="async" width="320" height="320" style="height:auto">
              <br><a href="https://sites.nd.edu/xiangliang-zhang/" style="font-size: 1.2em; color: #00356B;">Xiangliang Zhang</a></br>
              <p>Notre Dame</p>
            </div>
          </div>

          <div class="col-lg-2 col-md-3 col-sm-4 col-6">
            <div class="speaker" data-aos="fade-up" data-aos-delay="200" style="text-align: center;">
              <img src="assets/img/organizers/faculty/manling.jpg" alt="Manling Li" class="img-fluid"
                   loading="lazy" decoding="async" width="320" height="320" style="height:auto">
              <br><a href="https://limanling.github.io/" style="font-size: 1.2em; color: #00356B;">Manling Li</a></br>
              <p>Northwestern</p>
            </div>
          </div>

          <div class="col-lg-2 col-md-3 col-sm-4 col-6">
            <div class="speaker" data-aos="fade-up" data-aos-delay="200" style="text-align: center;">
              <img src="assets/img/organizers/faculty/yapeng.jpg" alt="Yapeng Tian" class="img-fluid"
                   loading="lazy" decoding="async" width="320" height="320" style="height:auto">
              <br><a href="https://www.yapengtian.com/" style="font-size: 1.2em; color: #00356B;">Yapeng Tian</a></br>
              <p>UT Dallas</p>
            </div>
          </div>

          <div class="col-lg-2 col-md-3 col-sm-4 col-6">
            <div class="speaker" data-aos="fade-up" data-aos-delay="200" style="text-align: center;">
              <img src="assets/img/organizers/faculty/minhao.jpg" alt="Minhao Cheng" class="img-fluid"
                   loading="lazy" decoding="async" width="320" height="320" style="height:auto">
              <br><a href="https://cmhcbb.github.io/" style="font-size: 1.2em; color: #00356B;">Minhao Cheng</a></br>
              <p>Penn State</p>
            </div>
          </div>

          <div class="col-lg-2 col-md-3 col-sm-4 col-6">
            <div class="speaker" data-aos="fade-up" data-aos-delay="200" style="text-align: center;">
              <img src="assets/img/organizers/faculty/zeynep.jpg" alt="Zeynep Akata" class="img-fluid"
                   loading="lazy" decoding="async" width="320" height="320" style="height:auto">
              <br><a href="https://www.eml-munich.de/people/zeynep-akata" style="font-size: 1.2em; color: #00356B;">Zeynep Akata</a></br>
              <p>TUM</p>
            </div>
          </div>

          <div class="col-lg-2 col-md-3 col-sm-4 col-6">
            <div class="speaker" data-aos="fade-up" data-aos-delay="200" style="text-align: center;">
              <img src="assets/img/organizers/faculty/wenhu.jpg" alt="Wenhu Chen" class="img-fluid"
                   loading="lazy" decoding="async" width="320" height="320" style="height:auto">
              <br><a href="https://wenhuchen.github.io/" style="font-size: 1.2em; color: #00356B;">Wenhu Chen</a></br>
              <p>UWaterloo</p>
            </div>
          </div>

          <div class="col-lg-2 col-md-3 col-sm-4 col-6">
            <div class="speaker" data-aos="fade-up" data-aos-delay="200" style="text-align: center;">
              <img src="assets/img/organizers/student/yilun.jpeg" alt="Yilun Zhao" class="img-fluid"
                   loading="lazy" decoding="async" width="320" height="320" style="height:auto">
              <br><a href="https://yilunzhao.github.io/" style="font-size: 1.2em; color: #00356B;">Yilun Zhao</a></br>
              <p>Yale</p>
            </div>
          </div>

          <div class="col-lg-2 col-md-3 col-sm-4 col-6">
            <div class="speaker" data-aos="fade-up" data-aos-delay="200" style="text-align: center;">
              <img src="assets/img/organizers/student/haowei.png" alt="Haowei Zhang" class="img-fluid"
                   loading="lazy" decoding="async" width="320" height="320" style="height:auto">
              <br><a href="https://haowei-freesky.github.io/" style="font-size: 1.2em; color: #00356B;">Haowei Zhang</a></br>
              <p>Fudan University</p>
            </div>
          </div>

          <div class="col-lg-2 col-md-3 col-sm-4 col-6">
            <div class="speaker" data-aos="fade-up" data-aos-delay="200" style="text-align: center;">
              <img src="assets/img/organizers/student/tianyu.png" alt="Tianyu Yang" class="img-fluid"
                   loading="lazy" decoding="async" width="320" height="320" style="height:auto">
              <br><a href="https://tianyu-yang-anna.github.io/" style="font-size: 1.2em; color: #00356B;">Tianyu Yang</a></br>
              <p>Notre Dame</p>
            </div>
          </div>

          <div class="col-lg-2 col-md-3 col-sm-4 col-6">
            <div class="speaker" data-aos="fade-up" data-aos-delay="200" style="text-align: center;">
              <img src="assets/img/organizers/student/zhenting.jpg" alt="Zhenting Qi" class="img-fluid"
                   loading="lazy" decoding="async" width="320" height="320" style="height:auto">
              <br><a href="https://zhentingqi.github.io/" style="font-size: 1.2em; color: #00356B;">Zhenting Qi</a></br>
              <p>Harvard</p>
            </div>
          </div>

          <div class="col-lg-2 col-md-3 col-sm-4 col-6">
            <div class="speaker" data-aos="fade-up" data-aos-delay="200" style="text-align: center;">
              <img src="assets/img/organizers/student/yuyang.jpg" alt="Yuyang Liu" class="img-fluid"
                   loading="lazy" decoding="async" width="320" height="320" style="height:auto">
              <br><a href="https://scholar.google.com/citations?user=sWSKvYUAAAAJ&hl=zh-CN" style="font-size: 1.2em; color: #00356B;">Yuyang Liu</a></br>
              <p>Peking University</p>
            </div>
          </div>

          <div class="col-lg-2 col-md-3 col-sm-4 col-6">
            <div class="speaker" data-aos="fade-up" data-aos-delay="200" style="text-align: center;">
              <img src="assets/img/organizers/student/zhiyuan.jpg" alt="Zhiyuan Hu" class="img-fluid"
                   loading="lazy" decoding="async" width="320" height="320" style="height:auto">
              <br><a href="https://zhiyuanhubj.github.io/" style="font-size: 1.2em; color: #00356B;">Zhiyuan Hu</a></br>
              <p>NUS</p>
            </div>
          </div>

          <div class="col-lg-2 col-md-3 col-sm-4 col-6">
            <div class="speaker" data-aos="fade-up" data-aos-delay="200" style="text-align: center;">
              <img src="assets/img/organizers/student/simeng.png" alt="Simeng Han" class="img-fluid"
                   loading="lazy" decoding="async" width="320" height="320" style="height:auto">
              <br><a href="https://sophiahan6.github.io/" style="font-size: 1.2em; color: #00356B;">Simeng Han</a></br>
              <p>Yale</p>
            </div>
          </div>

          <div class="col-lg-2 col-md-3 col-sm-4 col-6">
            <div class="speaker" data-aos="fade-up" data-aos-delay="200" style="text-align: center;">
              <img src="assets/img/organizers/student/rui.png" alt="Rui Xiao" class="img-fluid"
                   loading="lazy" decoding="async" width="320" height="320" style="height:auto">
              <br><a href="https://www.eml-munich.de/people/rui-xiao" style="font-size: 1.2em; color: #00356B;">Rui Xiao</a></br>
              <p>TUM</p>
            </div>
          </div>

          <div class="col-lg-2 col-md-3 col-sm-4 col-6">
            <div class="speaker" data-aos="fade-up" data-aos-delay="200" style="text-align: center;">
              <img src="assets/img/organizers/student/xiangru.jpg" alt="Xiangru Tang" class="img-fluid"
                   loading="lazy" decoding="async" width="320" height="320" style="height:auto">
              <br><a href="https://sophiahan6.github.io/" style="font-size: 1.2em; color: #00356B;">Xiangru Tang</a></br>
              <p>Yale</p>
            </div>
          </div>

          <div class="col-lg-2 col-md-3 col-sm-4 col-6">
            <div class="speaker" data-aos="fade-up" data-aos-delay="200" style="text-align: center;">
              <img src="assets/img/organizers/student/fan.jpg" alt="Fan Nie" class="img-fluid"
                   loading="lazy" decoding="async" width="320" height="320" style="height:auto">
              <br><a href="https://fannie1208.github.io/" style="font-size: 1.2em; color: #00356B;">Fan Nie</a></br>
              <p>Stanford</p>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End Organizers -->

  </main><!-- End #main -->

  <!-- ======= Sponsors Section ======= -->
  <section id="sponsors" class="section-with-bg">
    <div class="container">
      <div class="section-header">
        <h2>Sponsors</h2>
      </div>
      <div class="row justify-content-center">
        <div class="col-lg-4 col-md-6">
          <div class="sponsor text-center">
            <img src="assets/img/tencent.png" alt="Tencent" class="img-fluid"
                 loading="lazy" decoding="async" width="320" height="120" style="height:auto; max-height: 120px;">
          </div>
        </div>
      </div>
      <div class="row mt-4">
        <div class="col-12 text-center">
          <p><em>We welcome sponsorship opportunities. To become a sponsor, please contact us (Yilun Zhao: <a href="mailto:yilun.zhao@yale.edu">yilun.zhao@yale.edu</a>).</em></p>
        </div>
      </div>
    </div>
  </section><!-- End Sponsors Section -->

  <!-- ======= Footer ======= -->
  <footer id="footer">
    <div class="container">
      <div class="copyright">
        <!-- &copy; Copyright <strong>TheEvent</strong>. All Rights Reserved -->
      </div>
      <div class="credits">
        Template adopted from <a href="https://workshop-llm-reasoning-planning.github.io/" target="_blank" rel="noopener noreferrer">Reasoning and Planning for Large Language Models @ ICLR 2025</a>
      </div>
    </div>
  </footer><!-- End Footer -->

  <a href="#" class="back-to-top d-flex align-items-center justify-content-center" aria-label="Back to top"><i class="bi bi-arrow-up-short"></i></a>

  <!-- Vendor JS Files (deferred for faster first paint) -->
  <script src="assets/vendor/aos/aos.js" defer></script>
  <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js" defer></script>
  <script src="assets/vendor/glightbox/js/glightbox.min.js" defer></script>
  <script src="assets/vendor/swiper/swiper-bundle.min.js" defer></script>
  <!-- Removed php-email-form/validate.js because this page has no contact form -->

  <!-- Template Main JS File -->
  <script src="assets/js/main.js" defer></script>

  <!-- ======= Accepted Papers: load on page load ======= -->
  <script>
    // Load accepted_paper.json when the page loads
    document.addEventListener('DOMContentLoaded', function () {
      const list = document.getElementById('posterList');
      if (!list) return;

      fetch('accepted_paper.json')
        .then(function (r) { return r.json(); })
        .then(function (data) {
          const notes = (data && data.notes) ? data.notes : [];
          notes.forEach(function (item) {
            const title = item && item.content && item.content.title ? (item.content.title.value || 'Untitled') : 'Untitled';
            const authorsVal = item && item.content && item.content.authors ? item.content.authors.value : [];
            const authors = Array.isArray(authorsVal) ? authorsVal.join(', ') : (authorsVal || '');
            const li = document.createElement('li');
            li.innerHTML = '<b>' + title + '</b><br>' + authors;
            list.appendChild(li);
            list.appendChild(document.createElement('br'));
          });
        })
        .catch(function (err) {
          console.error('Error loading accepted_paper.json:', err);
        });
    });
  </script>

  <!-- ======= Lazy-load flag counter after full load to avoid blocking ======= -->
  <script>
    // English comments: Inject the counter only after window load
    window.addEventListener('load', function () {
      const a = document.createElement('a');
      a.href = 'https://info.flagcounter.com/fIO5';
      a.rel = 'noopener noreferrer';
      a.target = '_blank';
      a.innerHTML = '<img src="https://s05.flagcounter.com/count2/fIO5/bg_FFFFFF/txt_000000/border_CCCCCC/columns_6/maxflags_20/viewers_0/labels_0/pageviews_0/flags_0/percent_0/" alt="Flag Counter" border="0" width="480" height="80" loading="lazy" decoding="async">';
      document.body.appendChild(a);
    });
  </script>

</body>
</html>
