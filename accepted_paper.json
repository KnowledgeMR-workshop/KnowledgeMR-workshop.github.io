{
  "notes": [
    {
      "id": "On64WQKVzm",
      "number": 1,
      "forum": "On64WQKVzm",
      "content": {
        "title": {
          "value": "Generate, Transduct, Adapt: Iterative Transduction with VLMs"
        },
        "authors": {
          "value": [
            "Oindrila Saha",
            "Logan Christopher Lawrence",
            "Grant Van Horn",
            "Subhransu Maji"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission1/Authors"
          ]
        },
        "authorids": {
          "value": [
            "~Oindrila_Saha4",
            "~Logan_Christopher_Lawrence1",
            "~Grant_Van_Horn1",
            "~Subhransu_Maji1"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission1/Authors"
          ]
        },
        "keywords": {
          "value": [
            "Vision Language Models",
            "Unsupervised Learning"
          ]
        },
        "TLDR": {
          "value": "We introduce GTA-CLIP, a transductive zero-shot learning method that leverages both vision and language spaces for improved classification accuracy."
        },
        "abstract": {
          "value": "Transductive zero-shot learning with vision-language models leverages image-image similarities within the dataset to achieve better classification accuracy compared to the inductive setting. However, there is little work that explores the structure of the language space in this context. We propose GTA-CLIP, a novel technique that incorporates supervision from language models for joint transduction in language and vision spaces. Our approach is iterative and consists of three steps: (i) incrementally exploring the attribute space by querying language models, (ii) an attribute-augmented transductive inference procedure, and (iii) fine-tuning the language and vision encoders based on inferred labels within the dataset. Through experiments with CLIP encoders, we demonstrate that GTA-CLIP yields an average performance improvement of 9.5% and 4.0% across 12 datasets and 3 encoders, over CLIP and transductive CLIP respectively in the zero-shot setting. We also observe similar improvements in a few-shot setting. We present ablation studies that demonstrate the value of each step and visualize how the vision and language spaces evolve over iterations driven by the transductive learning."
        },
        "pdf": {
          "value": "/pdf/2ff4ae1f9df6a848cf3d4daba42c19b4707fa611.pdf"
        },
        "venue": {
          "value": "KnowledgeMR"
        },
        "venueid": {
          "value": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR"
        },
        "_bibtex": {
          "value": "@inproceedings{\nanonymous2025generate,\ntitle={Generate, Transduct, Adapt: Iterative Transduction with {VLM}s},\nauthor={Anonymous},\nbooktitle={1st Workshop on Knowledge-Intensive Multimodal Reasoning},\nyear={2025},\nurl={https://openreview.net/forum?id=On64WQKVzm}\n}"
        },
        "paperhash": {
          "value": "saha|generate_transduct_adapt_iterative_transduction_with_vlms",
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission1/Authors"
          ]
        }
      },
      "details": {
        "replies": [
          {
            "id": "4T6edBsNjh",
            "forum": "On64WQKVzm",
            "replyto": "On64WQKVzm",
            "signatures": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "nonreaders": [],
            "readers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission1/Authors"
            ],
            "writers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "content": {
              "title": {
                "value": "Paper Decision"
              },
              "decision": {
                "value": "Accept"
              },
              "comment": {
                "value": "This paper has been accepted to ICCV 2025, and we are delighted to feature its presentation in our workshop."
              }
            },
            "number": 1,
            "invitations": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission1/-/Decision",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit"
            ],
            "domain": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "tcdate": 1756687633512,
            "cdate": 1756687633512,
            "tmdate": 1756688396694,
            "mdate": 1756688396694,
            "parentInvitations": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Decision",
            "license": "CC BY 4.0",
            "version": 2
          },
          {
            "id": "g9bDBfweZh",
            "forum": "On64WQKVzm",
            "replyto": "4asNLFcJi7",
            "signatures": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "readers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission1/Senior_Area_Chairs",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission1/Area_Chairs",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission1/Reviewers",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission1/Authors"
            ],
            "writers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR"
            ],
            "content": {
              "revert_desk_rejection_confirmation": {
                "value": "We approve the reversion of desk-rejected submission."
              }
            },
            "number": 1,
            "invitations": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission1/-/Desk_Rejection_Reversion"
            ],
            "domain": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "tcdate": 1756528058187,
            "cdate": 1756528058187,
            "tmdate": 1756528058187,
            "mdate": 1756528058187,
            "parentInvitations": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Desk_Rejection_Reversion",
            "license": "CC BY 4.0",
            "version": 2
          },
          {
            "id": "4asNLFcJi7",
            "forum": "On64WQKVzm",
            "replyto": "On64WQKVzm",
            "signatures": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "readers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission1/Senior_Area_Chairs",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission1/Area_Chairs",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission1/Reviewers",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission1/Authors"
            ],
            "writers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR"
            ],
            "content": {
              "desk_reject_comments": {
                "value": "Regrettably, the paper does not comply with the author guidelines on template use and has therefore been desk rejected."
              },
              "title": {
                "value": "Submission Desk Rejected by Program Chairs"
              }
            },
            "number": 1,
            "invitations": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission1/-/Desk_Rejection"
            ],
            "domain": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "tcdate": 1756524471729,
            "cdate": 1756524471729,
            "tmdate": 1756524471729,
            "mdate": 1756524471729,
            "parentInvitations": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Desk_Rejection",
            "license": "CC BY 4.0",
            "version": 2
          }
        ]
      },
      "invitations": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Post_Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Desk_Rejected_Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit"
      ],
      "readers": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission1/Authors"
      ]
    },
    {
      "id": "sD4eeSouVA",
      "number": 2,
      "forum": "sD4eeSouVA",
      "content": {
        "title": {
          "value": "Video-MMLU: A Massive Multi-Discipline Lecture Understanding Benchmark<br><i>üèÜ Outstanding Paper Award (Benchmark Track)</i>"
        },
        "authors": {
          "value": [
            "Enxin Song",
            "Wenhao Chai",
            "Weili Xu",
            "Jianwen Xie",
            "Yuxuan Liu",
            "Gaoang Wang"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission2/Authors"
          ]
        },
        "authorids": {
          "value": [
            "~Enxin_Song2",
            "~Wenhao_Chai1",
            "~Weili_Xu1",
            "~Jianwen_Xie1",
            "~Yuxuan_Liu23",
            "~Gaoang_Wang2"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission2/Authors"
          ]
        },
        "keywords": {
          "value": [
            "Video Understanding",
            "Multi-Discipline Lecture Video",
            "Video Understanding Benchmark",
            "Video Question-Answering",
            "Video Captioning"
          ]
        },
        "TLDR": {
          "value": "Video-MMLU is a massive benchmark designed to evaluate the capabilities of LMMs in understanding Multi-Discipline Lectures."
        },
        "abstract": {
          "value": "Recent advancements in language multimodal models (LMMs) for video have demonstrated their potential for understanding video content, yet the task of comprehending multi-discipline lectures remains largely unexplored. We introduce Video-MMLU, a massive benchmark designed to evaluate the capabilities of LMMs in understanding Multi-Discipline Lectures. We evaluate over 90 open-source and proprietary models, ranging from 0.5B to 40B parameters. Our results highlight the limitations of current models in addressing the cognitive challenges presented by these lectures, especially in tasks requiring both perception and reasoning. Additionally, we explore how the number of visual tokens and the large language models influence performance, offering insights into the interplay between multimodal perception and reasoning in lecture comprehension."
        },
        "pdf": {
          "value": "/pdf/76cd6fa44eff022892830f1ba62b6fb450721c5f.pdf"
        },
        "venue": {
          "value": "KnowledgeMR"
        },
        "venueid": {
          "value": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR"
        },
        "_bibtex": {
          "value": "@inproceedings{\nanonymous2025videommlu,\ntitle={Video-{MMLU}: A Massive Multi-Discipline Lecture Understanding Benchmark},\nauthor={Anonymous},\nbooktitle={1st Workshop on Knowledge-Intensive Multimodal Reasoning},\nyear={2025},\nurl={https://openreview.net/forum?id=sD4eeSouVA}\n}"
        },
        "paperhash": {
          "value": "song|videommlu_a_massive_multidiscipline_lecture_understanding_benchmark",
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission2/Authors"
          ]
        }
      },
      "details": {
        "replies": [
          {
            "id": "WlhfnKxEjJ",
            "forum": "sD4eeSouVA",
            "replyto": "sD4eeSouVA",
            "signatures": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "nonreaders": [],
            "readers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission2/Authors"
            ],
            "writers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "content": {
              "title": {
                "value": "Paper Decision"
              },
              "decision": {
                "value": "Accept"
              },
              "comment": {
                "value": "The paper introduces Video-MMLU, a large-scale benchmark for lecture video comprehension across mathematics, physics, and chemistry. The dataset construction is thorough, and the evaluation of over 90 models provides valuable insights into the gap between perception and reasoning in LMMs. The work is well-motivated, clearly presented, and highly relevant to the workshop theme. I recommend acceptance."
              }
            },
            "number": 1,
            "invitations": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission2/-/Decision",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit"
            ],
            "domain": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "tcdate": 1756687632634,
            "cdate": 1756687632634,
            "tmdate": 1756688396806,
            "mdate": 1756688396806,
            "parentInvitations": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Decision",
            "license": "CC BY 4.0",
            "version": 2
          }
        ]
      },
      "invitations": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Post_Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit"
      ],
      "readers": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission2/Authors"
      ]
    },
    {
      "id": "pR5fWmpLCZ",
      "number": 4,
      "forum": "pR5fWmpLCZ",
      "content": {
        "title": {
          "value": "Thinking vs. Doing: Agents that Reason by  Scaling Test-Time Interaction"
        },
        "authors": {
          "value": [
            "Junhong Shen",
            "Hao Bai",
            "Lunjun Zhang",
            "Yifei Zhou",
            "Amrith Setlur",
            "Shengbang Tong",
            "Diego Caples",
            "Nan Jiang",
            "Tong Zhang",
            "Ameet Talwalkar",
            "Aviral Kumar"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission4/Authors"
          ]
        },
        "authorids": {
          "value": [
            "~Junhong_Shen1",
            "~Hao_Bai1",
            "~Lunjun_Zhang1",
            "~Yifei_Zhou1",
            "~Amrith_Setlur1",
            "~Shengbang_Tong1",
            "~Diego_Caples1",
            "~Nan_Jiang2",
            "~Tong_Zhang2",
            "~Ameet_Talwalkar1",
            "~Aviral_Kumar2"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission4/Authors"
          ]
        },
        "keywords": {
          "value": [
            "multi-modal web agent",
            "agent reasoning",
            "test-time scaling"
          ]
        },
        "TLDR": {
          "value": "We propose to scale the number of interaction steps for agents as a new axis of test-time scaling and develop a curriculum-based online RL algorithm for training agents to scale interaction."
        },
        "abstract": {
          "value": "The current paradigm of test-time scaling relies on generating long reasoning traces (\"thinking\" more) before producing a response. In agent problems that require interaction, this can be done by generating thinking traces before acting in the world. However, this process does not allow agents to acquire new information from the environment or adapt their behavior over time. In this work, we propose to scale test-time interaction, an untapped dimension of test-time scaling that increases the agent's interaction horizon to enable running rich behaviors such as exploration, backtracking, and dynamic re-planning within a single rollout. To demonstrate the promise of this scaling dimension, we study the domain of web agents. We first show that even prompting-based interaction scaling without any training can improve task success on web benchmarks. Building on this, we introduce TTI (Test-Time Interaction), a curriculum-based online reinforcement learning (RL) approach that trains agents by adaptively adjusting their rollout lengths. Using a Gemma 3 12B model, TTI produces state-of-the-art open-source, open-data, multi-modal web agents on WebVoyager and WebArena benchmarks. Our results establish interaction scaling as a  complementary axis to scaling per-step compute, offering new avenues for  adaptive agent training."
        },
        "pdf": {
          "value": "/pdf/b1126581513550debcdca5ef3e2b2cc2eadaa55a.pdf"
        },
        "venue": {
          "value": "KnowledgeMR"
        },
        "venueid": {
          "value": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR"
        },
        "_bibtex": {
          "value": "@inproceedings{\nanonymous2025thinking,\ntitle={Thinking vs. Doing: Agents that Reason by  Scaling Test-Time Interaction},\nauthor={Anonymous},\nbooktitle={1st Workshop on Knowledge-Intensive Multimodal Reasoning},\nyear={2025},\nurl={https://openreview.net/forum?id=pR5fWmpLCZ}\n}"
        },
        "paperhash": {
          "value": "shen|thinking_vs_doing_agents_that_reason_by_scaling_testtime_interaction",
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission4/Authors"
          ]
        }
      },
      "details": {
        "replies": [
          {
            "id": "CyHSAtSkJz",
            "forum": "pR5fWmpLCZ",
            "replyto": "pR5fWmpLCZ",
            "signatures": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "nonreaders": [],
            "readers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission4/Authors"
            ],
            "writers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "content": {
              "title": {
                "value": "Paper Decision"
              },
              "decision": {
                "value": "Accept"
              },
              "comment": {
                "value": "This paper introduces Test-Time Interaction (TTI), a novel dimension of scaling that extends an agent‚Äôs interaction horizon rather than just per-step reasoning. The proposed curriculum RL approach enables adaptive exploration and achieves state-of-the-art results on WebVoyager and WebArena using an open-weight model. The idea is well-motivated, the experiments thorough, and the contribution timely and relevant to interactive agent research. I recommend acceptance."
              }
            },
            "number": 1,
            "invitations": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission4/-/Decision",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit"
            ],
            "domain": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "tcdate": 1756687632712,
            "cdate": 1756687632712,
            "tmdate": 1756688396891,
            "mdate": 1756688396891,
            "parentInvitations": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Decision",
            "license": "CC BY 4.0",
            "version": 2
          }
        ]
      },
      "invitations": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Post_Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit"
      ],
      "readers": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission4/Authors"
      ]
    },
    {
      "id": "OnAomCQciG",
      "number": 6,
      "forum": "OnAomCQciG",
      "content": {
        "title": {
          "value": "VAGUE: Visual Contexts Clarify Ambiguous Expressions"
        },
        "authors": {
          "value": [
            "Heejeong Nam",
            "Jinwoo Ahn",
            "Keummin Ka",
            "Jiwan Chung",
            "Youngjae Yu"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission6/Authors"
          ]
        },
        "authorids": {
          "value": [
            "~Heejeong_Nam1",
            "~Jinwoo_Ahn3",
            "~Keummin_Ka1",
            "~Jiwan_Chung1",
            "~Youngjae_Yu1"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission6/Authors"
          ]
        },
        "keywords": {
          "value": [
            "Multimodal reasoning",
            "Benchmark Dataset",
            "Evaluation"
          ]
        },
        "TLDR": {
          "value": "VAGUE introduces a multimodal benchmark of 1,600 ambiguous utterances grounded in real images and shows that state-of-the-art LVLMs fail to resolve indirect, visual context-dependent expressions."
        },
        "abstract": {
          "value": "Human communication often relies on visual cues to resolve ambiguity. While humans can intuitively integrate these cues, AI systems often find it challenging to engage in sophisticated multimodal reasoning. We introduce VAGUE, a benchmark evaluating multimodal AI systems' ability to integrate visual context for intent disambiguation. VAGUE consists of 1.6K ambiguous textual expressions, each paired with an image and multiple-choice interpretations, where the correct answer is only apparent with visual context. The dataset spans both staged, complex (Visual Commonsense Reasoning) and natural, personal (Ego4D) scenes, ensuring diversity.\nOur experiments reveal that existing multimodal AI models struggle to infer the speaker's true intent. While performance consistently improves from the introduction of more visual cues, the overall accuracy remains far below human performance, highlighting a critical gap in multimodal reasoning. Analysis of failure cases demonstrates that current models fail to distinguish true intent from superficial correlations in the visual scene, indicating that they perceive images but do not effectively reason with them."
        },
        "pdf": {
          "value": "/pdf/2eb703a82a50cb1d2ff3d5cda1a9b56346e32e54.pdf"
        },
        "venue": {
          "value": "KnowledgeMR"
        },
        "venueid": {
          "value": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR"
        },
        "_bibtex": {
          "value": "@inproceedings{\nanonymous2025vague,\ntitle={{VAGUE}: Visual Contexts Clarify Ambiguous Expressions},\nauthor={Anonymous},\nbooktitle={1st Workshop on Knowledge-Intensive Multimodal Reasoning},\nyear={2025},\nurl={https://openreview.net/forum?id=OnAomCQciG}\n}"
        },
        "paperhash": {
          "value": "nam|vague_visual_contexts_clarify_ambiguous_expressions",
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission6/Authors"
          ]
        }
      },
      "details": {
        "replies": [
          {
            "id": "Jbh6w1VVuy",
            "forum": "OnAomCQciG",
            "replyto": "OnAomCQciG",
            "signatures": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "nonreaders": [],
            "readers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission6/Authors"
            ],
            "writers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "content": {
              "title": {
                "value": "Paper Decision"
              },
              "decision": {
                "value": "Accept"
              },
              "comment": {
                "value": "This paper introduces VAGUE, a benchmark for multimodal intention disambiguation with 1.6K ambiguous expressions paired with images. Results show that current models perceive visual cues but fail to integrate them for intent reasoning, highlighting a critical gap in multimodal ToM. The dataset is original, well-curated, and highly relevant. I recommend acceptance."
              }
            },
            "number": 1,
            "invitations": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission6/-/Decision",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit"
            ],
            "domain": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "tcdate": 1756687632713,
            "cdate": 1756687632713,
            "tmdate": 1756688397041,
            "mdate": 1756688397041,
            "parentInvitations": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Decision",
            "license": "CC BY 4.0",
            "version": 2
          }
        ]
      },
      "invitations": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Post_Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission6/-/Camera_Ready"
      ],
      "readers": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission6/Authors"
      ]
    },
    {
      "id": "SV2HsehpYs",
      "number": 7,
      "forum": "SV2HsehpYs",
      "content": {
        "title": {
          "value": "A Comprehensive Analysis for Visual Object Hallucination in Large Vision-Language Models"
        },
        "authors": {
          "value": [
            "Liqiang Jing",
            "Hardy Chen",
            "Ehsan Aghazadeh",
            "Xin Eric Wang",
            "Xinya Du"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission7/Authors"
          ]
        },
        "authorids": {
          "value": [
            "~Liqiang_Jing1",
            "~Hardy_Chen1",
            "~Ehsan_Aghazadeh1",
            "~Xin_Eric_Wang2",
            "~Xinya_Du1"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission7/Authors"
          ]
        },
        "keywords": {
          "value": [
            "Analysis",
            "Hallucination",
            "Multimodal Models"
          ]
        },
        "abstract": {
          "value": "Large Vision-Language Models (LVLMs) demonstrate remarkable capabilities in multimodal tasks, but visual object hallucination remains a persistent issue. It refers to scenarios where models generate inaccurate visual object-related information based on the query input, potentially leading to misinformation and concerns about safety and reliability. Previous works focus on the evaluation and mitigation of visual hallucinations, but the underlying causes have not been comprehensively investigated. In this paper, we analyze each component of LLaVA-like LVLMs‚Äîthe large language model, the vision backbone, and the projector, to identify potential sources of error and their impact. Based on our observations, we propose methods to mitigate hallucination for each problematic component. Additionally, we developed two hallucination benchmarks: QA-VisualGenome, which emphasizes attribute and relation hallucinations, and QA-FB15k, which focuses on cognition-based hallucinations."
        },
        "pdf": {
          "value": "/pdf/82f9e31a22d2106fb22e22f70f9801d18aee7a55.pdf"
        },
        "venue": {
          "value": "KnowledgeMR"
        },
        "venueid": {
          "value": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR"
        },
        "_bibtex": {
          "value": "@inproceedings{\nanonymous2025a,\ntitle={A Comprehensive Analysis for Visual Object Hallucination in Large Vision-Language Models},\nauthor={Anonymous},\nbooktitle={1st Workshop on Knowledge-Intensive Multimodal Reasoning},\nyear={2025},\nurl={https://openreview.net/forum?id=SV2HsehpYs}\n}"
        },
        "paperhash": {
          "value": "jing|a_comprehensive_analysis_for_visual_object_hallucination_in_large_visionlanguage_models",
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission7/Authors"
          ]
        }
      },
      "details": {
        "replies": [
          {
            "id": "T3q5F6PWlD",
            "forum": "SV2HsehpYs",
            "replyto": "SV2HsehpYs",
            "signatures": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "nonreaders": [],
            "readers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission7/Authors"
            ],
            "writers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "content": {
              "title": {
                "value": "Paper Decision"
              },
              "decision": {
                "value": "Accept"
              },
              "comment": {
                "value": "This paper makes a strong and timely contribution by offering the first comprehensive component-level analysis of visual object hallucination in large vision-language models. It systematically disentangles the roles of the LLM, vision encoder, and projector, then introduces targeted mitigation strategies that meaningfully reduce hallucinations. Importantly, the authors develop two new benchmarks that expand evaluation beyond prior work. The combination of rigorous analysis and valuable benchmarking resources makes this paper deserving of acceptance"
              }
            },
            "number": 1,
            "invitations": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission7/-/Decision",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit"
            ],
            "domain": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "tcdate": 1756687633516,
            "cdate": 1756687633516,
            "tmdate": 1756688397045,
            "mdate": 1756688397045,
            "parentInvitations": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Decision",
            "license": "CC BY 4.0",
            "version": 2
          }
        ]
      },
      "invitations": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Post_Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit"
      ],
      "readers": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission7/Authors"
      ]
    },
    {
      "id": "DsSzi4sHY0",
      "number": 9,
      "forum": "DsSzi4sHY0",
      "content": {
        "title": {
          "value": "WikiVideo: Article Generation from Multiple Videos"
        },
        "authors": {
          "value": [
            "Alexander Martin",
            "Reno Kriz",
            "William Gantt Walden",
            "Kate Sanders",
            "Hannah Recknor",
            "Eugene Yang",
            "Francis Ferraro",
            "Benjamin Van Durme"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission9/Authors"
          ]
        },
        "authorids": {
          "value": [
            "~Alexander_Martin1",
            "~Reno_Kriz1",
            "~William_Gantt_Walden1",
            "~Kate_Sanders1",
            "~Hannah_Recknor1",
            "~Eugene_Yang2",
            "~Francis_Ferraro1",
            "~Benjamin_Van_Durme2"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission9/Authors"
          ]
        },
        "keywords": {
          "value": [
            "video understanding",
            "article generation",
            "multimodal RAG"
          ]
        },
        "TLDR": {
          "value": "writing wikipedia style articles from multiple videos"
        },
        "abstract": {
          "value": "We introduce the task of grounded article generation with the goal of creating a Wikipedia-style article from multiple diverse videos about real-world events‚Äîfrom natural disasters to political elections‚Äîwhere all the information in the article is supported by video evidence. Videos are intuitive sources for retrieval-augmented generation (RAG), but most contemporary RAG workflows focus heavily on text while existing methods for video-based summarization focus on low-level scene understanding rather than high-level event semantics. To close this gap, we introduce WikiVideo, a benchmark consisting of expert-written articles and densely annotated videos that provide evidence for articles‚Äô claims, facilitating the integration of video into RAG pipelines and enabling the creation of in-depth content that is grounded in multimodal sources. We further propose Collaborative Article Generation (CAG), a novel interactive method for article creation from multiple videos. CAG leverages an iterative interaction between an r1-style reasoning model and a VideoLLM to draw higher-level inferences about the target event than is possible with VideoLLMs alone, which fixate on low-level visual features. We benchmark state-of-the-art VideoLLMs and CAG in both oracle retrieval and RAG settings and find that CAG consistently outperforms alternative methods, while suggesting intriguing avenues for future work"
        },
        "pdf": {
          "value": "/pdf/a52f5afda5d326c91d4415377fd929d56558cf9b.pdf"
        },
        "venue": {
          "value": "KnowledgeMR"
        },
        "venueid": {
          "value": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR"
        },
        "_bibtex": {
          "value": "@inproceedings{\nanonymous2025wikivideo,\ntitle={WikiVideo: Article Generation from Multiple Videos},\nauthor={Anonymous},\nbooktitle={1st Workshop on Knowledge-Intensive Multimodal Reasoning},\nyear={2025},\nurl={https://openreview.net/forum?id=DsSzi4sHY0}\n}"
        },
        "paperhash": {
          "value": "martin|wikivideo_article_generation_from_multiple_videos",
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission9/Authors"
          ]
        }
      },
      "details": {
        "replies": [
          {
            "id": "4BPPNJVXkf",
            "forum": "DsSzi4sHY0",
            "replyto": "DsSzi4sHY0",
            "signatures": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "nonreaders": [],
            "readers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission9/Authors"
            ],
            "writers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "content": {
              "title": {
                "value": "Paper Decision"
              },
              "decision": {
                "value": "Accept"
              },
              "comment": {
                "value": "This paper makes a strong and timely contribution by introducing WIKIVIDEO, the first benchmark for generating Wikipedia-style articles grounded in multiple videos, and proposing Collaborative Article Generation (CAG), a novel method that integrates reasoning with VideoLLMs for high-level event synthesis. This work opens an important new direction in multimodal article generation and merits acceptance"
              }
            },
            "number": 1,
            "invitations": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission9/-/Decision",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit"
            ],
            "domain": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "tcdate": 1756687633595,
            "cdate": 1756687633595,
            "tmdate": 1756688397206,
            "mdate": 1756688397206,
            "parentInvitations": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Decision",
            "license": "CC BY 4.0",
            "version": 2
          }
        ]
      },
      "invitations": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Post_Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit"
      ],
      "readers": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission9/Authors"
      ]
    },
    {
      "id": "HmoqQRYeWu",
      "number": 11,
      "forum": "HmoqQRYeWu",
      "content": {
        "title": {
          "value": "Flex-Judge: Text-Only Reasoning Unleashes Zero-Shot Multimodal Evaluators"
        },
        "authors": {
          "value": [
            "Jongwoo Ko",
            "Sungnyun Kim",
            "Sungwoo Cho",
            "Se-Young Yun"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission11/Authors"
          ]
        },
        "authorids": {
          "value": [
            "~Jongwoo_Ko1",
            "~Sungnyun_Kim1",
            "~Sungwoo_Cho1",
            "~Se-Young_Yun1"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission11/Authors"
          ]
        },
        "keywords": {
          "value": [
            "mllm-as-a-judge",
            "automatic evaluation",
            "multimodal reasoning",
            "multimodal large language models"
          ]
        },
        "TLDR": {
          "value": "We propose Flex-Judge, a multimodal judge model trained solely on a small corpus of high-quality text reasoning data."
        },
        "abstract": {
          "value": "Human-generated reward signals are critical for aligning generative models with human preferences, guiding both training and inference-time evaluations. While large language models (LLMs) employed as proxy evaluators, i.e., LLM-as-a-Judge, significantly reduce the costs associated with manual annotations, they typically require extensive modality-specific training data and fail to generalize well across diverse multimodal tasks. In this paper, we propose Flex-Judge, a reasoning-guided multimodal judge model that leverages minimal textual reasoning data to robustly generalize across multiple modalities and evaluation formats. Our core intuition is that structured textual reasoning explanations inherently encode generalizable decision-making patterns, enabling an effective transfer to multimodal judgments, e.g., with images or videos. Empirical results demonstrate that Flex-Judge, despite being trained on significantly fewer text data, achieves competitive or superior performance compared to state-of-the-art commercial APIs and extensively trained multimodal evaluators. Notably, Flex-Judge presents broad impact in modalities like molecule, where comprehensive evaluation benchmarks are scarce, underscoring its practical value in resource-constrained domains. Our framework highlights reasoning-based text supervision as a powerful, cost-effective alternative to traditional annotation-intensive approaches, substantially advancing scalable multimodal model-as-a-judge."
        },
        "pdf": {
          "value": "/pdf/24f9f3a268331134c193e78567db4e6a8bd36e57.pdf"
        },
        "venue": {
          "value": "KnowledgeMR"
        },
        "venueid": {
          "value": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR"
        },
        "_bibtex": {
          "value": "@inproceedings{\nanonymous2025flexjudge,\ntitle={Flex-Judge: Text-Only Reasoning Unleashes Zero-Shot Multimodal Evaluators},\nauthor={Anonymous},\nbooktitle={1st Workshop on Knowledge-Intensive Multimodal Reasoning},\nyear={2025},\nurl={https://openreview.net/forum?id=HmoqQRYeWu}\n}"
        },
        "paperhash": {
          "value": "ko|flexjudge_textonly_reasoning_unleashes_zeroshot_multimodal_evaluators",
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission11/Authors"
          ]
        }
      },
      "details": {
        "replies": [
          {
            "id": "4rqKM1Jsuf",
            "forum": "HmoqQRYeWu",
            "replyto": "HmoqQRYeWu",
            "signatures": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "nonreaders": [],
            "readers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission11/Authors"
            ],
            "writers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "content": {
              "title": {
                "value": "Paper Decision"
              },
              "decision": {
                "value": "Accept"
              },
              "comment": {
                "value": "This paper presents FLEX-Judge, a reasoning-guided multimodal evaluator trained on only a small set of text reasoning data, yet demonstrating strong generalization across images, video, audio, and molecular tasks. The work‚Äôs central insight that structured textual rationales can transfer effectively across modalities yields both cost efficiency and superior performance relative to large-scale commercial and open-source baselines. With impressive results on diverse benchmarks and impactful applications in scientific domains, this paper represents a significant, practical contribution meriting acceptance"
              }
            },
            "number": 1,
            "invitations": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission11/-/Decision",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit"
            ],
            "domain": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "tcdate": 1756687633646,
            "cdate": 1756687633646,
            "tmdate": 1756688397571,
            "mdate": 1756688397571,
            "parentInvitations": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Decision",
            "license": "CC BY 4.0",
            "version": 2
          }
        ]
      },
      "invitations": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Post_Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission11/-/Camera_Ready"
      ],
      "readers": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission11/Authors"
      ]
    },
    {
      "id": "dBZek6p4s6",
      "number": 12,
      "forum": "dBZek6p4s6",
      "content": {
        "title": {
          "value": "Zero-shot Copyright Risk Assessment of Character Designs via Vision-Language Reasoning"
        },
        "authors": {
          "value": [
            "Seongsoon Kim",
            "Jinseop Shin",
            "Seongchan Kim",
            "Minki Kim",
            "Wonsu Kim"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission12/Authors"
          ]
        },
        "authorids": {
          "value": [
            "~Seongsoon_Kim1",
            "js.shin@kisti.re.kr",
            "~Seongchan_Kim1",
            "mk.kim@kisti.re.kr",
            "wonsukim@kisti.re.kr"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission12/Authors"
          ]
        },
        "keywords": {
          "value": [
            "Vision-Language Models",
            "Copyright Infringement Assessment",
            "Multimodal Legal Reasoning"
          ]
        },
        "TLDR": {
          "value": "We propose a prompt-based ViLM framework that mimics legal reasoning to assess visual copyright infringement, achieving high accuracy on real-world trademark disputes."
        },
        "abstract": {
          "value": "Copyright and trademark disputes are increasing due to growing conflicts over visual intellectual property (IP). These cases often involve subjective judgment and specialized legal expertise, making visual similarity assessment both time-consuming and difficult to standardize. This study investigates whether Vision-Language Models (ViLMs) can support structured reasoning about creative expression, a key factor in copyright infringement analysis. We propose a multi-step evaluation protocol that guides ViLMs through abstraction, filtration of generic elements, and comparison of expressive features, reflecting legal principles such as the Abstraction‚ÄìFiltration‚ÄìComparison (AFC) test. Based on this framework, we develop a ViLM-based system that analyzes character and trademark images using staged prompts. We validate the method on a dataset of real-world legal disputes from South Korea and compare its performance with a CLIP-based similarity baseline. Experimental results show that ViLM achieves 87.10\\% accuracy on a six-point similarity scale and an F1 score of 0.9697 in a binary setting, outperforming CLIP in identifying legally meaningful expressive content. These results suggest that ViLMs can support expert-level reasoning in copyright analysis and may be useful in other knowledge-intensive multimodal applications."
        },
        "pdf": {
          "value": "/pdf/ba2ae1365b44ab93654fcc858378aabd2878c13a.pdf"
        },
        "venue": {
          "value": "KnowledgeMR"
        },
        "venueid": {
          "value": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR"
        },
        "_bibtex": {
          "value": "@inproceedings{\nanonymous2025zeroshot,\ntitle={Zero-shot Copyright Risk Assessment of Character Designs via Vision-Language Reasoning},\nauthor={Anonymous},\nbooktitle={1st Workshop on Knowledge-Intensive Multimodal Reasoning},\nyear={2025},\nurl={https://openreview.net/forum?id=dBZek6p4s6}\n}"
        },
        "paperhash": {
          "value": "kim|zeroshot_copyright_risk_assessment_of_character_designs_via_visionlanguage_reasoning",
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission12/Authors"
          ]
        }
      },
      "details": {
        "replies": [
          {
            "id": "HIjZUev8wG",
            "forum": "dBZek6p4s6",
            "replyto": "dBZek6p4s6",
            "signatures": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "nonreaders": [],
            "readers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission12/Authors"
            ],
            "writers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "content": {
              "title": {
                "value": "Paper Decision"
              },
              "decision": {
                "value": "Accept"
              },
              "comment": {
                "value": "The paper proposes a ViLM-based framework guided by the Abstraction‚ÄìFiltration‚ÄìComparison test for copyright risk assessment. Evaluations on real trademark disputes show high accuracy and interpretability, outperforming CLIP. The study bridges AI and law, offering both rigor and societal impact. I recommend acceptance."
              }
            },
            "number": 1,
            "invitations": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission12/-/Decision",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit"
            ],
            "domain": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "tcdate": 1756687632754,
            "cdate": 1756687632754,
            "tmdate": 1756688397794,
            "mdate": 1756688397794,
            "parentInvitations": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Decision",
            "license": "CC BY 4.0",
            "version": 2
          }
        ]
      },
      "invitations": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Post_Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission12/-/Camera_Ready"
      ],
      "readers": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission12/Authors"
      ]
    },
    {
      "id": "QYZYGeCu6v",
      "number": 14,
      "forum": "QYZYGeCu6v",
      "content": {
        "title": {
          "value": "REVEAL: Robust Evolution of Vision-Language Models for Explainable AI-Video Detection"
        },
        "authors": {
          "value": [
            "Yun-Yun Tsai",
            "Qingyuan Liu",
            "Ruijian Zha",
            "Victoria Li",
            "Pengyuan Shi",
            "Chengzhi Mao",
            "Junfeng Yang"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission14/Authors"
          ]
        },
        "authorids": {
          "value": [
            "~Yun-Yun_Tsai1",
            "~Qingyuan_Liu4",
            "~Ruijian_Zha1",
            "~Victoria_Li1",
            "~Pengyuan_Shi1",
            "~Chengzhi_Mao2",
            "~Junfeng_Yang1"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission14/Authors"
          ]
        },
        "keywords": {
          "value": [
            "LVLM reasoning",
            "Ai-generated video detection"
          ]
        },
        "TLDR": {
          "value": "We introduce REVEAL, a tool-integrated vision-language model for detecting AI-generated videos with high interpretability and robustness."
        },
        "abstract": {
          "value": "The rapid advancement of AI-generated video poses challenges to digital authenticity and security. Current detection methods, often trained on specific datasets, struggle with the ever-evolving landscape of generative techniques and unseen manipulations. We introduce a framework leveraging Vision Language Models (VLMs) for robust AI-generated video detection. Our approach equips the VLM with the ability to reason about video content and use external tools to identify subtle inconsistencies, mirroring human system 2 thinking. Our self-evolving VLM dynamically selects and composes appropriate tools, enhancing its ability to generalize to novel video generation techniques. The modular design promotes interpretability, allowing for a clearer understanding of VLM's decision-making process. To evaluate, we establish the first benchmark \\vidfor containing 1.4k+ high-quality AI generated videos across eight generative models. Experiments show that \\reveal improves F1 scores by 9.1% to 30.2% over top baselines across our datasets for VLMs, notably for GPT-4o, Gemini 1.5 pro, and QWen-VL-Max, and Llava-One-Vision-7B. While open-world AI-video detection remains an open challenge, our results indicate that existing methods fail primarily because they lack tool-enabled, higher-order reasoning."
        },
        "pdf": {
          "value": "/pdf/b9e4339208a7f1ad49660deaf0c834352a26385f.pdf"
        },
        "venue": {
          "value": "KnowledgeMR"
        },
        "venueid": {
          "value": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR"
        },
        "_bibtex": {
          "value": "@inproceedings{\nanonymous2025reveal,\ntitle={{REVEAL}: Robust Evolution of Vision-Language Models for Explainable {AI}-Video Detection},\nauthor={Anonymous},\nbooktitle={1st Workshop on Knowledge-Intensive Multimodal Reasoning},\nyear={2025},\nurl={https://openreview.net/forum?id=QYZYGeCu6v}\n}"
        },
        "paperhash": {
          "value": "tsai|reveal_robust_evolution_of_visionlanguage_models_for_explainable_aivideo_detection",
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission14/Authors"
          ]
        }
      },
      "details": {
        "replies": [
          {
            "id": "l8dAyclAk3",
            "forum": "QYZYGeCu6v",
            "replyto": "QYZYGeCu6v",
            "signatures": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "nonreaders": [],
            "readers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission14/Authors"
            ],
            "writers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "content": {
              "title": {
                "value": "Paper Decision"
              },
              "decision": {
                "value": "Accept"
              },
              "comment": {
                "value": "This work presents REVEAL, a self-evolving VLM framework for AI-generated video detection with explicit tool use and structured prompts. Evaluations on the VidForensic dataset show large F1 improvements over baselines. The approach is modular, interpretable, and effective against evolving generation techniques. I recommend acceptance."
              }
            },
            "number": 1,
            "invitations": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission14/-/Decision",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit"
            ],
            "domain": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "tcdate": 1756687632830,
            "cdate": 1756687632830,
            "tmdate": 1756688398078,
            "mdate": 1756688398078,
            "parentInvitations": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Decision",
            "license": "CC BY 4.0",
            "version": 2
          }
        ]
      },
      "invitations": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Post_Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit"
      ],
      "readers": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission14/Authors"
      ]
    },
    {
      "id": "fDjFjaIVXH",
      "number": 15,
      "forum": "fDjFjaIVXH",
      "content": {
        "title": {
          "value": "MaRVL: A Benchmark for Mathematical Reasoning over Visual Landscapes"
        },
        "authors": {
          "value": [
            "Nilay Pande",
            "Sahiti Yerramilli",
            "Jayant Sravan Tamarapalli",
            "Rynaa Grover"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission15/Authors"
          ]
        },
        "authorids": {
          "value": [
            "~Nilay_Pande2",
            "~Sahiti_Yerramilli1",
            "~Jayant_Sravan_Tamarapalli1",
            "~Rynaa_Grover1"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission15/Authors"
          ]
        },
        "keywords": {
          "value": [
            "Mathematical Visual Reasoning",
            "Abstract & Structural Reasoning",
            "MLLM Evaluation & Failure Analysis",
            "AI Benchmarking",
            "Geometric & Topological Reasoning",
            "Model Robustness & Generalization"
          ]
        },
        "abstract": {
          "value": "A key frontier for Multimodal Large Language Models (MLLMs) is the ability to perform deep mathematical and spatial reasoning directly from images, moving beyond their established success in semantic description. Mathematical surface plots provide a rigorous testbed for this capability, as they isolate the task of reasoning from the semantic noise common in natural images. To measure progress on this frontier, we introduce MaRVL (Mathematical Reasoning over Visual Landscapes), a new benchmark designed to quantitatively evaluate these core reasoning skills. The benchmark comprises two novel tasks: Topological Counting, identifying and enumerating features like local maxima; and Transformation Recognition, recognizing applied geometric transformations. Generated from a curated library of functions with rigorous ambiguity filtering, our evaluation on MaRVL reveals that even state-of-the-art MLLMs struggle significantly, often resorting to superficial heuristics instead of robust spatial reasoning. MaRVL provides a challenging new tool for the research community to measure progress, expose model limitations, and guide the development of MLLMs with more profound reasoning abilities."
        },
        "pdf": {
          "value": "/pdf/e05e73408399f477e510621e183f9989c44daabd.pdf"
        },
        "venue": {
          "value": "KnowledgeMR"
        },
        "venueid": {
          "value": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR"
        },
        "_bibtex": {
          "value": "@inproceedings{\nanonymous2025marvl,\ntitle={Ma{RVL}: A Benchmark for Mathematical Reasoning over Visual Landscapes},\nauthor={Anonymous},\nbooktitle={1st Workshop on Knowledge-Intensive Multimodal Reasoning},\nyear={2025},\nurl={https://openreview.net/forum?id=fDjFjaIVXH}\n}"
        },
        "paperhash": {
          "value": "pande|marvl_a_benchmark_for_mathematical_reasoning_over_visual_landscapes",
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission15/Authors"
          ]
        }
      },
      "details": {
        "replies": [
          {
            "id": "zc2IXjmL4p",
            "forum": "fDjFjaIVXH",
            "replyto": "fDjFjaIVXH",
            "signatures": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "nonreaders": [],
            "readers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission15/Authors"
            ],
            "writers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "content": {
              "title": {
                "value": "Paper Decision"
              },
              "decision": {
                "value": "Accept"
              },
              "comment": {
                "value": "This paper introduces MaRVL, a carefully designed benchmark for assessing mathematical and spatial reasoning in multimodal large language models through semantically sparse function plots. The benchmark includes two novel tasks, Topological Counting and Transformation Recognition, generated from a rigorously curated library of functions and filtered for unambiguous evaluation. The authors contribute both a large dataset and a balanced MaRVL-Mini test set, demonstrating that state-of-the-art models still struggle. I recommend acceptance"
              }
            },
            "number": 1,
            "invitations": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission15/-/Decision",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit"
            ],
            "domain": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "tcdate": 1756687633818,
            "cdate": 1756687633818,
            "tmdate": 1756688398495,
            "mdate": 1756688398495,
            "parentInvitations": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Decision",
            "license": "CC BY 4.0",
            "version": 2
          }
        ]
      },
      "invitations": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Post_Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit"
      ],
      "readers": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission15/Authors"
      ]
    },
    {
      "id": "FdY9jHxlXA",
      "number": 16,
      "forum": "FdY9jHxlXA",
      "content": {
        "title": {
          "value": "CountQA: How Well Do MLLMs Count in the Wild?"
        },
        "authors": {
          "value": [
            "Jayant Sravan Tamarapalli",
            "Rynaa Grover",
            "Nilay Pande",
            "Sahiti Yerramilli"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission16/Authors"
          ]
        },
        "authorids": {
          "value": [
            "~Jayant_Sravan_Tamarapalli1",
            "~Rynaa_Grover1",
            "~Nilay_Pande2",
            "~Sahiti_Yerramilli1"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission16/Authors"
          ]
        },
        "keywords": {
          "value": [
            "Counting",
            "Multimodal",
            "Spatial Reasoning"
          ]
        },
        "TLDR": {
          "value": "This paper introduces CountQA, a new benchmark designed to test object counting in multimodal large language models (MLLMs), and finds that even top-performing models are surprisingly poor at this task."
        },
        "abstract": {
          "value": "Multimodal Large Language Models (MLLMs) demonstrate remarkable fluency in understanding visual scenes, yet they exhibit a critical failure in a fundamental cognitive skill: object counting. This blind spot severely limits their reliability in real-world applications. To date, this capability has been largely unevaluated in complex scenarios, as existing benchmarks either feature sparse object densities or are confined to specific visual domains, failing to test models under realistic conditions. Addressing this gap, we introduce CountQA, a challenging new benchmark designed to probe this deficiency. Comprising over 1,500 question-answer pairs, CountQA features real-world images with high object density, clutter, and occlusion. Our extensive evaluation of 15 prominent MLLMs reveals a startling inability to quantify objects: the top-performing model achieves a mere 42.9% accuracy, with performance declining as object counts rise. By providing a dedicated benchmark to diagnose and rectify this core weakness, CountQA paves the way for a new generation of MLLMs that are not only descriptively fluent but also numerically grounded and spatially aware."
        },
        "pdf": {
          "value": "/pdf/eef4cc980eb189b09de98bd07d246e05086e96e6.pdf"
        },
        "venue": {
          "value": "KnowledgeMR"
        },
        "venueid": {
          "value": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR"
        },
        "_bibtex": {
          "value": "@inproceedings{\nanonymous2025countqa,\ntitle={Count{QA}: How Well Do {MLLM}s Count in the Wild?},\nauthor={Anonymous},\nbooktitle={1st Workshop on Knowledge-Intensive Multimodal Reasoning},\nyear={2025},\nurl={https://openreview.net/forum?id=FdY9jHxlXA}\n}"
        },
        "paperhash": {
          "value": "tamarapalli|countqa_how_well_do_mllms_count_in_the_wild",
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission16/Authors"
          ]
        }
      },
      "details": {
        "replies": [
          {
            "id": "txhM5k7EEj",
            "forum": "FdY9jHxlXA",
            "replyto": "FdY9jHxlXA",
            "signatures": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "nonreaders": [],
            "readers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission16/Authors"
            ],
            "writers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "content": {
              "title": {
                "value": "Paper Decision"
              },
              "decision": {
                "value": "Accept"
              },
              "comment": {
                "value": "The paper introduces CountQA, a benchmark of 1.5K QA pairs on dense, cluttered real-world images. Evaluations across 15 MLLMs reveal severe weaknesses in object counting, with top models under 43% accuracy. The benchmark fills an essential gap and will catalyze research on numerically grounded MLLMs. I recommend acceptance. "
              }
            },
            "number": 1,
            "invitations": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission16/-/Decision",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit"
            ],
            "domain": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "tcdate": 1756687632854,
            "cdate": 1756687632854,
            "tmdate": 1756688398510,
            "mdate": 1756688398510,
            "parentInvitations": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Decision",
            "license": "CC BY 4.0",
            "version": 2
          }
        ]
      },
      "invitations": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Post_Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit"
      ],
      "readers": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission16/Authors"
      ]
    },
    {
      "id": "fgXPaEKvRo",
      "number": 17,
      "forum": "fgXPaEKvRo",
      "content": {
        "title": {
          "value": "Enhancing Region-Level Reasoning in Vision-Language Models with Complementary Visual Focus"
        },
        "authors": {
          "value": [
            "Soojin Jang",
            "Kwanyong Park",
            "Ilchae Jung",
            "Yong-Ju Lee"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission17/Authors"
          ]
        },
        "authorids": {
          "value": [
            "~Soojin_Jang1",
            "~Kwanyong_Park1",
            "~Ilchae_Jung2",
            "~Yong-Ju_Lee2"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission17/Authors"
          ]
        },
        "keywords": {
          "value": [
            "vision-language model",
            "multimodal reasoning",
            "chain-of-thought reasoning"
          ]
        },
        "abstract": {
          "value": "Recent advances in chain-of-thought (CoT) reasoning have improved the interpretability of vision-language models (VLMs), but they still struggle with region-level reasoning. Existing methods such as visual prompting or fine-tuning demand costly supervision. In this work, we propose a training-free approach for robust region-level reasoning that avoids fine-tuning or additional data collection. Our method generates complementary visual contexts‚Äîincluding region-marked, region-masked, and region-focused views‚Äîfrom a single image, encouraging diverse reasoning by shifting the model's visual focus. Token-level predictions from diverse visual contexts are fused into a unified logit that captures complementary cues, enabling self-verifying and reliable next-token generation. Evaluated on the ViP-Bench benchmark, our method significantly outperforms both baseline CoT models and existing training-free and fine-tuned approaches. These results demonstrate the effectiveness of multi-view reasoning for enhancing region-level and multimodal understanding in VLMs, without requiring any task-specific retraining."
        },
        "pdf": {
          "value": "/pdf/6474dfab1e9aa0628bac1ae7b5892f06e94d6bed.pdf"
        },
        "venue": {
          "value": "KnowledgeMR"
        },
        "venueid": {
          "value": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR"
        },
        "_bibtex": {
          "value": "@inproceedings{\nanonymous2025enhancing,\ntitle={Enhancing Region-Level Reasoning in Vision-Language Models with Complementary Visual Focus},\nauthor={Anonymous},\nbooktitle={1st Workshop on Knowledge-Intensive Multimodal Reasoning},\nyear={2025},\nurl={https://openreview.net/forum?id=fgXPaEKvRo}\n}"
        },
        "paperhash": {
          "value": "jang|enhancing_regionlevel_reasoning_in_visionlanguage_models_with_complementary_visual_focus",
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission17/Authors"
          ]
        }
      },
      "details": {
        "replies": [
          {
            "id": "xgK30lJjbP",
            "forum": "fgXPaEKvRo",
            "replyto": "fgXPaEKvRo",
            "signatures": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "nonreaders": [],
            "readers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission17/Authors"
            ],
            "writers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "content": {
              "title": {
                "value": "Paper Decision"
              },
              "decision": {
                "value": "Accept"
              },
              "comment": {
                "value": "This submission presents a simple, training-free method that markedly improves region-level reasoning in VLMs by generating complementary visual contexts (marked, masked, focused) and fusing token-level logits via a weighted mixer to guide self-verifying generation. On ViP-Bench it consistently surpasses baseline CoT models, prior training-free prompts, and fine-tuned systems, with clear ablations showing the value of each view and the fusion strategy. I recommend acceptance."
              }
            },
            "number": 1,
            "invitations": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission17/-/Decision",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit"
            ],
            "domain": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "tcdate": 1756687633816,
            "cdate": 1756687633816,
            "tmdate": 1756688398871,
            "mdate": 1756688398871,
            "parentInvitations": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Decision",
            "license": "CC BY 4.0",
            "version": 2
          }
        ]
      },
      "invitations": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Post_Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission17/-/Camera_Ready"
      ],
      "readers": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission17/Authors"
      ]
    },
    {
      "id": "rqy08bWM5C",
      "number": 18,
      "forum": "rqy08bWM5C",
      "content": {
        "title": {
          "value": "Exploring Pretrained Diffusion Representation for Visual-Language Models"
        },
        "authors": {
          "value": [
            "Ilchae Jung",
            "Soojin Jang",
            "Youngwan Lee",
            "Yong-Ju Lee"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission18/Authors"
          ]
        },
        "authorids": {
          "value": [
            "~Ilchae_Jung2",
            "~Soojin_Jang1",
            "~Youngwan_Lee1",
            "~Yong-Ju_Lee2"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission18/Authors"
          ]
        },
        "keywords": {
          "value": [
            "vision-language model",
            "multimodal representation",
            "diffusion model"
          ]
        },
        "abstract": {
          "value": "Recent advances in vision and large language models (VLMs) have been improved due to enhanced computational resources and remarkable research breakthroughs. Despite this rapid progress, effectively integrating visual and textual information remains a key challenge in multimodal representation learning. In this work, we explore whether pretrained text-to-image diffusion models‚Äîoriginally trained on aligned image-text pairs‚Äîcan serve as a complementary representation of multimodal knowledge for VLMs. We propose Diffusion-Guided LLaVA, called DG-LLaVA, a simple yet effective framework that enhances multimodal understanding by fusing representations from a vision-language model and a pretrained diffusion-based model. DG-LLaVA introduces a prompt rewriting module and a simple yet effective fine-tuning framework, leveraging pretrained off-the-shelf models. We validate the effectiveness of DG-LLaVA through extensive experiments on multiple benchmarks, demonstrating improvements in multimodal understanding tasks."
        },
        "pdf": {
          "value": "/pdf/10e2e7c976c01d51411e4d913983a794f2a5d4ff.pdf"
        },
        "venue": {
          "value": "KnowledgeMR"
        },
        "venueid": {
          "value": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR"
        },
        "_bibtex": {
          "value": "@inproceedings{\nanonymous2025exploring,\ntitle={Exploring Pretrained Diffusion Representation for Visual-Language Models},\nauthor={Anonymous},\nbooktitle={1st Workshop on Knowledge-Intensive Multimodal Reasoning},\nyear={2025},\nurl={https://openreview.net/forum?id=rqy08bWM5C}\n}"
        },
        "paperhash": {
          "value": "jung|exploring_pretrained_diffusion_representation_for_visuallanguage_models",
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission18/Authors"
          ]
        }
      },
      "details": {
        "replies": [
          {
            "id": "JMlxRXsvl3",
            "forum": "rqy08bWM5C",
            "replyto": "rqy08bWM5C",
            "signatures": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "nonreaders": [],
            "readers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission18/Authors"
            ],
            "writers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "content": {
              "title": {
                "value": "Paper Decision"
              },
              "decision": {
                "value": "Accept"
              },
              "comment": {
                "value": "This paper proposes DG-LLaVA, which fuses diffusion-model representations with VLMs via prompt rewriting and a linking projector. Experiments show consistent improvements on multimodal understanding benchmarks. The method is simple, modular, and effective, leveraging pretrained models efficiently. I recommend acceptance."
              }
            },
            "number": 1,
            "invitations": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission18/-/Decision",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit"
            ],
            "domain": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "tcdate": 1756687632863,
            "cdate": 1756687632863,
            "tmdate": 1756688398939,
            "mdate": 1756688398939,
            "parentInvitations": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Decision",
            "license": "CC BY 4.0",
            "version": 2
          }
        ]
      },
      "invitations": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Post_Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission18/-/Camera_Ready"
      ],
      "readers": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission18/Authors"
      ]
    },
    {
      "id": "ZMNZqm87ls",
      "number": 19,
      "forum": "ZMNZqm87ls",
      "content": {
        "title": {
          "value": "You May Speak Freely: Improving the Fine-Grained Visual Recognition Capabilities of Multimodal Large Language Models with Answer Extraction"
        },
        "authors": {
          "value": [
            "Logan Lawrence",
            "Oindrila Saha",
            "Megan Wei",
            "Chen Sun",
            "Subhransu Maji",
            "Grant Van Horn"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission19/Authors"
          ]
        },
        "authorids": {
          "value": [
            "~Logan_Lawrence1",
            "~Oindrila_Saha4",
            "~Megan_Wei1",
            "~Chen_Sun1",
            "~Subhransu_Maji1",
            "~Grant_Van_Horn1"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission19/Authors"
          ]
        },
        "keywords": {
          "value": [
            "fine-grained classification",
            "retrieval",
            "VQA",
            "multimodal large language models"
          ]
        },
        "TLDR": {
          "value": "Answer extraction with constrained decoding using out-of-the-box MLLMs performs well in full-way fine-grained visual classification, where the choice counts are in the hundreds to thousands."
        },
        "abstract": {
          "value": "Despite the renewed interest in zero-shot visual classification due to the rise of Multimodal Large Language Models (MLLMs), the problem of evaluating free-form responses of autoregressive models remains a persistent challenge. Most existing works focus on language-only tasks or don't consider Multiple Choice Questions (MCQs) beyond 5-way options, both of which are critical capabilities to solve tasks in Fine-Grained Visual Classification (FGVC) where choice counts are in the hundreds to thousands and the choices are highly related. Furthermore, in this highly multi-way MCQ setting it is not clear how to extend LLM choice extraction to retrieval-based problems, where computing probabilities over the choice set is computationally costly. In this work we investigate \\textit{nlg2choice}, a simple two-stage method which first asks the MLLM an open-ended question for the task with minimal constraints, then uses text-only constrained decoding to predict the most likely choice. In retrieval settings, we compute the probability of the constrained response taking that choice with an early stopping method to significantly improve throughput. Our results show improvement over a suite of seven fine-grained visual datasets when evaluating in terms of classification and retrieval, and show that this performance holds over the various ways that users of LLMs can implement tasks in natural language."
        },
        "pdf": {
          "value": "/pdf/427c6d23faeec480b2e4dddbc60f68c7f8073819.pdf"
        },
        "venue": {
          "value": "KnowledgeMR"
        },
        "venueid": {
          "value": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR"
        },
        "_bibtex": {
          "value": "@inproceedings{\nanonymous2025you,\ntitle={You May Speak Freely: Improving the Fine-Grained Visual Recognition Capabilities of Multimodal Large Language Models with Answer Extraction},\nauthor={Anonymous},\nbooktitle={1st Workshop on Knowledge-Intensive Multimodal Reasoning},\nyear={2025},\nurl={https://openreview.net/forum?id=ZMNZqm87ls}\n}"
        },
        "paperhash": {
          "value": "lawrence|you_may_speak_freely_improving_the_finegrained_visual_recognition_capabilities_of_multimodal_large_language_models_with_answer_extraction",
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission19/Authors"
          ]
        }
      },
      "details": {
        "replies": [
          {
            "id": "cleEgSdo5B",
            "forum": "ZMNZqm87ls",
            "replyto": "ZMNZqm87ls",
            "signatures": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "nonreaders": [],
            "readers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission19/Authors"
            ],
            "writers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "content": {
              "title": {
                "value": "Paper Decision"
              },
              "decision": {
                "value": "Accept"
              },
              "comment": {
                "value": "The paper proposes nlg2choice, a two stage approach that first elicits free form answers from MLLMs, then uses text only constrained decoding to map them to fine grained classes. Across seven FGVC datasets it delivers consistent gains in classification and retrieval, improved genus level alignment of errors, and robustness to prompt variation. An early stopping strategy speeds probability computation, making the approach practical and broadly useful to the community. I recommend acceptance. "
              }
            },
            "number": 1,
            "invitations": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission19/-/Decision",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit"
            ],
            "domain": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "tcdate": 1756687633911,
            "cdate": 1756687633911,
            "tmdate": 1756688398981,
            "mdate": 1756688398981,
            "parentInvitations": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Decision",
            "license": "CC BY 4.0",
            "version": 2
          }
        ]
      },
      "invitations": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Post_Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit"
      ],
      "readers": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission19/Authors"
      ]
    },
    {
      "id": "mZ5RgIYaux",
      "number": 20,
      "forum": "mZ5RgIYaux",
      "content": {
        "title": {
          "value": "Seeing Isn‚Äôt Telling: Knowledge-Based Guidance Is All You Need"
        },
        "authors": {
          "value": [
            "Jiwon Kim",
            "Changhun Lee",
            "Chiehyeon Lim"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission20/Authors"
          ]
        },
        "authorids": {
          "value": [
            "~Jiwon_Kim11",
            "~Changhun_Lee1",
            "~Chiehyeon_Lim1"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission20/Authors"
          ]
        },
        "keywords": {
          "value": [
            "medical report generation",
            "knowledge-based guidance",
            "lesion-aware anatomical object detector",
            "enhanced disease-aware prompt",
            "differential attention",
            "healthcare"
          ]
        },
        "abstract": {
          "value": "In recent years, automated medical report generation (MRG) has gained significant research value for its potential to reduce workload and prevent diagnostic errors. Despite recent advances, generating accurate radiology reports remains challenging due to subtle visual cues the decoder's bias toward frequent disease patterns, and toward generic and repetitive expressions. \nIn this work, we propose an effective framework comprising three main components designed to explicitly ground the decoder in clinically relevant visual features. First, to identify and localize disease-specific regions, we employ a lesion-aware object detector. Second, to alleviate decoder's bias toward frequent disease patterns, we integrate a more robust disease classification module trained with an unlikelihood loss to suppressing false negatives. Third, to mitigate decoder's bias toward generic textual patterns, we adopt a differential text decoder that enhances the model sensitivity to diagnostically relevant words. \nWe retain the original image-report pairs from the MIMIC-CXR dataset and enhance the learning by incorporating external knowledge (i.e., anatomical regions) from the Imagenome dataset---a structured, data-driven external source of image-level annotations. Preliminary results demonstrate that leveraging anatomical regions and their abnormality information from external supervision and disease prompt lead to substantial improves in accurate and plausible report generation."
        },
        "pdf": {
          "value": "/pdf/1d7ca025de4dddb737c656136644e171ee09584a.pdf"
        },
        "venue": {
          "value": "KnowledgeMR"
        },
        "venueid": {
          "value": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR"
        },
        "TLDR": {
          "value": "Leveraging lesion and disease knowledge helps mitigate challenges from subtle visual cues and decoder bias."
        },
        "_bibtex": {
          "value": "@inproceedings{\nanonymous2025seeing,\ntitle={Seeing Isn{\\textquoteright}t Telling: Knowledge-Based Guidance Is All You Need},\nauthor={Anonymous},\nbooktitle={1st Workshop on Knowledge-Intensive Multimodal Reasoning},\nyear={2025},\nurl={https://openreview.net/forum?id=mZ5RgIYaux}\n}"
        },
        "paperhash": {
          "value": "kim|seeing_isnt_telling_knowledgebased_guidance_is_all_you_need",
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission20/Authors"
          ]
        }
      },
      "details": {
        "replies": [
          {
            "id": "14TcZI8fE6",
            "forum": "mZ5RgIYaux",
            "replyto": "mZ5RgIYaux",
            "signatures": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "nonreaders": [],
            "readers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission20/Authors"
            ],
            "writers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "content": {
              "title": {
                "value": "Paper Decision"
              },
              "decision": {
                "value": "Accept"
              },
              "comment": {
                "value": "This paper proposes a framework for medical report generation that grounds decoders in clinically relevant features using lesion detection, robust disease prompts, and differential attention. Evaluations on MIMIC-CXR show improved accuracy and plausibility. The work is well-motivated, addresses key challenges in visual grounding, and demonstrates strong clinical relevance. I recommend acceptance."
              }
            },
            "number": 1,
            "invitations": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission20/-/Decision",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit"
            ],
            "domain": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "tcdate": 1756687633013,
            "cdate": 1756687633013,
            "tmdate": 1756688399160,
            "mdate": 1756688399160,
            "parentInvitations": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Decision",
            "license": "CC BY 4.0",
            "version": 2
          }
        ]
      },
      "invitations": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Post_Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit"
      ],
      "readers": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission20/Authors"
      ]
    },
    {
      "id": "5Y8KFG4LgH",
      "number": 21,
      "forum": "5Y8KFG4LgH",
      "content": {
        "title": {
          "value": "Bayesian Multimodal Fusion for Scientific Figure Understanding: Adapting CLIP with Uncertainty-Aware Figure-Text Alignment"
        },
        "authors": {
          "value": [
            "Laura Tran-Dubois"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission21/Authors"
          ]
        },
        "authorids": {
          "value": [
            "~Laura_Tran-Dubois1"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission21/Authors"
          ]
        },
        "keywords": {
          "value": [
            "Bayesian deep learning",
            "multimodal retrieval",
            "scientific figure understanding",
            "uncertainty quantification",
            "vision-language models",
            "medical imaging",
            "document analysis"
          ]
        },
        "TLDR": {
          "value": "Bayesian multimodal framework for scientific figures that outperforms CLIP-style models by 4.9% accuracy with uncertainty-aware embeddings, reducing calibration error 56% while maintaining efficiency."
        },
        "abstract": {
          "value": "This paper presents a Bayesian multimodal framework for scientific figure understanding that addresses key limitations in current vision-language models. We propose probabilistic embeddings that explicitly model uncertainty in figure-caption relationships through learned covariance matrices, enabling reliable confidence estimates for retrieval tasks. Our method adapts CLIP-style architectures with novel training objectives for uncertainty-aware similarity computation, outperforming deterministic baselines by 4.9\\% accuracy on medical imaging and reducing calibration error by 56\\%. Experiments across three scientific benchmarks (SciCap, ChartQA, PubMed-CXR) demonstrate consistent improvements in retrieval accuracy, robustness to image corruption, and human trust scores. The framework's computational efficiency and domain adaptability make it practical for real-world scientific document analysis."
        },
        "pdf": {
          "value": "/pdf/85317a8043cf5d9cfddf965feb2977e6d91ba1ea.pdf"
        },
        "venue": {
          "value": "KnowledgeMR"
        },
        "venueid": {
          "value": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR"
        },
        "_bibtex": {
          "value": "@inproceedings{\nanonymous2025bayesian,\ntitle={Bayesian Multimodal Fusion for Scientific Figure Understanding: Adapting {CLIP} with Uncertainty-Aware Figure-Text Alignment},\nauthor={Anonymous},\nbooktitle={1st Workshop on Knowledge-Intensive Multimodal Reasoning},\nyear={2025},\nurl={https://openreview.net/forum?id=5Y8KFG4LgH}\n}"
        },
        "paperhash": {
          "value": "trandubois|bayesian_multimodal_fusion_for_scientific_figure_understanding_adapting_clip_with_uncertaintyaware_figuretext_alignment",
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission21/Authors"
          ]
        }
      },
      "details": {
        "replies": [
          {
            "id": "DBQNsjH4hC",
            "forum": "5Y8KFG4LgH",
            "replyto": "5Y8KFG4LgH",
            "signatures": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "nonreaders": [],
            "readers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission21/Authors"
            ],
            "writers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "content": {
              "title": {
                "value": "Paper Decision"
              },
              "decision": {
                "value": "Accept"
              },
              "comment": {
                "value": "This submission offers a principled Bayesian multimodal framework for scientific figure understanding, modeling image and text as Gaussian embeddings and computing similarity via KL divergence. The method adapts CLIP and SciBERT with uncertainty-aware objectives and domain-adaptive pretraining, directly targeting reliability in figure‚Äìcaption retrieval.\n\nThe diagonal covariance assumption may limit cross-dimensional uncertainty; exploring low-rank or full-covariance variants could yield gains. The 3:1 science to general corpus mix is reasonable yet deserves a sensitivity study. Training shows a small per-epoch overhead; variance-reduction or sampling strategies could further reduce cost. The human study is promising; replication with larger and more diverse practitioners would strengthen external validity and interface guidance. I recommend boaderline accept.\n\n"
              }
            },
            "number": 1,
            "invitations": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission21/-/Decision",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit"
            ],
            "domain": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "tcdate": 1756687633991,
            "cdate": 1756687633991,
            "tmdate": 1756688399194,
            "mdate": 1756688399194,
            "parentInvitations": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Decision",
            "license": "CC BY 4.0",
            "version": 2
          }
        ]
      },
      "invitations": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Post_Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission21/-/Camera_Ready"
      ],
      "readers": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission21/Authors"
      ]
    },
    {
      "id": "ZeIAhycwkl",
      "number": 22,
      "forum": "ZeIAhycwkl",
      "content": {
        "title": {
          "value": "CHECK-MAT: A Benchmark for Evaluating Knowledge-Intensive Mathematical Reasoning in VLMs"
        },
        "authors": {
          "value": [
            "Ruslan Khrulev"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission22/Authors"
          ]
        },
        "authorids": {
          "value": [
            "~Ruslan_Khrulev1"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission22/Authors"
          ]
        },
        "keywords": {
          "value": [
            "Benchmark",
            "Mathematical Reasoning",
            "Vision-Language Models (VLMs)",
            "Automated Assessment",
            "Symbolic Reasoning",
            "Error Analysis"
          ]
        },
        "TLDR": {
          "value": "This paper introduces CHECK-MAT, a benchmark to assess if AI is ready for the high-stakes, socially-sensitive task of grading real handwritten math exams"
        },
        "abstract": {
          "value": "The ability to perform multi-step mathematical reasoning is a grand challenge for modern AI, particularly in knowledge-intensive domains. While benchmarks like MATH or GSM8K test problem-solving, a critical gap exists in evaluating the ability of Vision-Language Models (VLMs) to verify and grade human-generated, handwritten solutions ‚Äî a task requiring the fusion of visual perception with deep, domain-specific knowledge of mathematical principles and grading rubrics. To address this, we introduce CHECK-MAT, a new diagnostic benchmark designed to rigorously probe these knowledge-intensive reasoning capabilities of VLMs. Composed of 122 real-world, handwritten solutions from a high-stakes national exam, CHECK-MAT requires models to apply an expert-provided knowledge base (the grading rubric) to identify logical flaws. Our evaluation of seven state-of-the-art VLMs reveals systematic failure modes, particularly in parsing complex handwritten notation and in applying the knowledge required for geometric and algebraic reasoning chains. Our work not only establishes a challenging new benchmark for the community but also provides crucial insights into the limitations of current multimodal models in knowledge-intensive scenarios. You can find code in https://github.com/Karifannaa/Auto-check-EGE-math."
        },
        "pdf": {
          "value": "/pdf/399149914ccddb749e4e1ec0ed72e6fd977f2598.pdf"
        },
        "venue": {
          "value": "KnowledgeMR"
        },
        "venueid": {
          "value": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR"
        },
        "_bibtex": {
          "value": "@inproceedings{\nanonymous2025checkmat,\ntitle={{CHECK}-{MAT}: A Benchmark for Evaluating Knowledge-Intensive Mathematical Reasoning in {VLM}s},\nauthor={Anonymous},\nbooktitle={1st Workshop on Knowledge-Intensive Multimodal Reasoning},\nyear={2025},\nurl={https://openreview.net/forum?id=ZeIAhycwkl}\n}"
        },
        "paperhash": {
          "value": "khrulev|checkmat_a_benchmark_for_evaluating_knowledgeintensive_mathematical_reasoning_in_vlms",
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission22/Authors"
          ]
        }
      },
      "details": {
        "replies": [
          {
            "id": "8QReLa6r0e",
            "forum": "ZeIAhycwkl",
            "replyto": "ZeIAhycwkl",
            "signatures": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "nonreaders": [],
            "readers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission22/Authors"
            ],
            "writers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "content": {
              "title": {
                "value": "Paper Decision"
              },
              "decision": {
                "value": "Accept"
              },
              "comment": {
                "value": "The paper introduces CHECK-MAT, a benchmark evaluating VLMs on grading handwritten math solutions using expert rubrics. Derived from a national exam, it highlights models‚Äô struggles with notation parsing and logical verification. I recommend acceptance."
              }
            },
            "number": 1,
            "invitations": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission22/-/Decision",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit"
            ],
            "domain": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "tcdate": 1756687633024,
            "cdate": 1756687633024,
            "tmdate": 1756688399311,
            "mdate": 1756688399311,
            "parentInvitations": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Decision",
            "license": "CC BY 4.0",
            "version": 2
          }
        ]
      },
      "invitations": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Post_Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission22/-/Camera_Ready"
      ],
      "readers": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission22/Authors"
      ]
    },
    {
      "id": "lqsSY5DFcD",
      "number": 23,
      "forum": "lqsSY5DFcD",
      "content": {
        "title": {
          "value": "Passing the Driving Knowledge Test"
        },
        "authors": {
          "value": [
            "Maolin Wei",
            "Wanzhou Liu",
            "Eshed Ohn-Bar"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission23/Authors"
          ]
        },
        "authorids": {
          "value": [
            "~Maolin_Wei1",
            "~Wanzhou_Liu1",
            "~Eshed_Ohn-Bar4"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission23/Authors"
          ]
        },
        "keywords": {
          "value": [
            "Autonomous Driving",
            "Traffic Rules",
            "Multi-modal Large Language Model",
            "Visual Question Answering"
          ]
        },
        "abstract": {
          "value": "If a Large Language Model (LLM) were to take a driving knowledge test today, would it pass? Beyond standard spatial and visual question-answering (QA) tasks on current autonomous driving benchmarks, driving knowledge tests require a complete understanding of all traffic rules, signage, and right-of-way principles. To pass this test, human drivers must discern various edge cases that rarely appear in real-world datasets. In this work, we present DriveQA, an extensive open-source text and vision-based benchmark that exhaustively covers traffic regulations and scenarios. Through our experiments using DriveQA, we show that (1) state-of-the-art LLMs and Multimodal LLMs (MLLMs) perform well on basic traffic rules but exhibit significant weaknesses in numerical reasoning and complex right-of-way scenarios, traffic sign variations, and spatial layouts, (2) fine-tuning on DriveQA improves accuracy across multiple categories, particularly in regulatory sign recognition and intersection decision-making, (3) controlled variations in DriveQA-V provide insights into model sensitivity to environmental factors such as lighting, perspective, distance, and weather conditions, and (4) pretraining on DriveQA enhances downstream driving task performance, leading to improved results on real-world datasets such as nuScenes and BDD, while also demonstrating that models can internalize text and synthetic traffic knowledge to generalize effectively across downstream QA tasks."
        },
        "pdf": {
          "value": "/pdf/96f543f7f63f1735d9ea0697716dac4aa6851410.pdf"
        },
        "venue": {
          "value": "KnowledgeMR"
        },
        "venueid": {
          "value": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR"
        },
        "_bibtex": {
          "value": "@inproceedings{\nanonymous2025passing,\ntitle={Passing the Driving Knowledge Test},\nauthor={Anonymous},\nbooktitle={1st Workshop on Knowledge-Intensive Multimodal Reasoning},\nyear={2025},\nurl={https://openreview.net/forum?id=lqsSY5DFcD}\n}"
        },
        "paperhash": {
          "value": "wei|passing_the_driving_knowledge_test",
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission23/Authors"
          ]
        }
      },
      "details": {
        "replies": [
          {
            "id": "o3RemW24Z0",
            "forum": "lqsSY5DFcD",
            "replyto": "lqsSY5DFcD",
            "signatures": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "nonreaders": [],
            "readers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission23/Authors"
            ],
            "writers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "content": {
              "title": {
                "value": "Paper Decision"
              },
              "decision": {
                "value": "Accept"
              },
              "comment": {
                "value": "This submission introduces DriveQA, a comprehensive, open-source benchmark for driving knowledge that unifies text-only and multimodal QA across 19 categories and 220 signs, totaling ~474K samples. The authors contribute a carefully constructed dataset with controllable visual variations, a thorough evaluation of leading LLMs/MLLMs with CoT and RAG, and compelling evidence that fine-tuning on DriveQA improves regulatory understanding and transfers to real-world tasks, including Mapillary sign recognition, BDD-OIA reasoning, and nuScenes trajectory prediction. The benchmark fills an important gap in assessing rule compliance beyond perception and will serve as a valuable, scalable resource for the community. I recommend acceptance"
              }
            },
            "number": 1,
            "invitations": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission23/-/Decision",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit"
            ],
            "domain": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "tcdate": 1756687634092,
            "cdate": 1756687634092,
            "tmdate": 1756688399964,
            "mdate": 1756688399964,
            "parentInvitations": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Decision",
            "license": "CC BY 4.0",
            "version": 2
          }
        ]
      },
      "invitations": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Post_Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission23/-/Camera_Ready"
      ],
      "readers": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission23/Authors"
      ]
    },
    {
      "id": "SuBmW2bOLT",
      "number": 24,
      "forum": "SuBmW2bOLT",
      "content": {
        "title": {
          "value": "Are LLMs Generalist Hanabi Agents?"
        },
        "authors": {
          "value": [
            "Mahesh Ramesh",
            "Aswinkumar Ramkumar",
            "Pavan Thodima",
            "Kaousheik Jayakumar",
            "Aniket Rege"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission24/Authors"
          ]
        },
        "authorids": {
          "value": [
            "~Mahesh_Ramesh1",
            "~Aswinkumar_Ramkumar1",
            "~Pavan_Thodima1",
            "~Kaousheik_Jayakumar1",
            "~Aniket_Rege1"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission24/Authors"
          ]
        },
        "keywords": {
          "value": [
            "Cooperative Agents",
            "Multi-turn RL"
          ]
        },
        "abstract": {
          "value": "Cooperative reasoning under incomplete information is a significant challenge for both humans and multi‚Äëagent AI. The card game Hanabi embodies this challenge, demanding theory of mind reasoning and strategic communication. We present the largest evaluation to date of Large Language Models (LLMs) as Hanabi playing agents, assessing 17 state‚Äëof‚Äëthe‚Äëart LLMs in 2 to 5‚Äëplayer cooperative multi-agent settings. Agents were provided a minimal ‚ÄúMinCon‚Äù prompt and a context-rich ‚ÄúDeductCon‚Äù prompt that scaffolds reasoning with explicit card deductions motivated by Bayesian inference and strategic guidance, revealing that different prompts induced fundamentally different gameplay strategies. With the DeductCon prompt, the strongest reasoning models exceed 15 points out of 25 on average across all player counts, yet they still trail experienced human players and purpose‚Äëbuilt RL agents, both of which consistently score above 20. We perform systematic ablations with context engineering, Best‚Äëof‚ÄëK sampling, and multi‚Äëagent scaffolding to reveal when context helps, when sampling hurts, and why multi-agent coordination failures persist. To encourage further research in multi-agent play for Hanabi, we release two resources: (1) 1,520 full game logs for instruction tuning and (2) 560 games with dense move‚Äëlevel value annotations (rewards) for all candidate moves to enable Reinforcement Learning from AI Feedback (RLAIF) in cooperative settings."
        },
        "pdf": {
          "value": "/pdf/28d9a0e4096622f238038ece8f112e01ec883df7.pdf"
        },
        "venue": {
          "value": "KnowledgeMR"
        },
        "venueid": {
          "value": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR"
        },
        "_bibtex": {
          "value": "@inproceedings{\nanonymous2025are,\ntitle={Are {LLM}s Generalist Hanabi Agents?},\nauthor={Anonymous},\nbooktitle={1st Workshop on Knowledge-Intensive Multimodal Reasoning},\nyear={2025},\nurl={https://openreview.net/forum?id=SuBmW2bOLT}\n}"
        },
        "paperhash": {
          "value": "ramesh|are_llms_generalist_hanabi_agents",
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission24/Authors"
          ]
        }
      },
      "details": {
        "replies": [
          {
            "id": "g9yk1yvFol",
            "forum": "SuBmW2bOLT",
            "replyto": "SuBmW2bOLT",
            "signatures": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "nonreaders": [],
            "readers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission24/Authors"
            ],
            "writers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "content": {
              "title": {
                "value": "Paper Decision"
              },
              "decision": {
                "value": "Accept"
              },
              "comment": {
                "value": "This work evaluates 17 LLMs as cooperative agents in Hanabi across 2‚Äì5 player settings. Results show reasoning prompts significantly improve play, though models still lag behind humans and RL agents. The dataset contributions (game logs, reward annotations) are valuable for future research. The study is timely, rigorous, and highly relevant for multi-agent reasoning. I recommend acceptance."
              }
            },
            "number": 1,
            "invitations": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission24/-/Decision",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit"
            ],
            "domain": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "tcdate": 1756687633099,
            "cdate": 1756687633099,
            "tmdate": 1756688399499,
            "mdate": 1756688399499,
            "parentInvitations": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Decision",
            "license": "CC BY 4.0",
            "version": 2
          }
        ]
      },
      "invitations": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Post_Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission24/-/Camera_Ready"
      ],
      "readers": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission24/Authors"
      ]
    },
    {
      "id": "a3ESrzK5S6",
      "number": 25,
      "forum": "a3ESrzK5S6",
      "content": {
        "title": {
          "value": "TRACE: Textual Reasoning for Affordance Coordinate Extraction"
        },
        "authors": {
          "value": [
            "Sangyun Park",
            "Jin Kim",
            "Yuchen Cui",
            "Matthew Sherman Brown"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission25/Authors"
          ]
        },
        "authorids": {
          "value": [
            "~Sangyun_Park2",
            "~Jin_Kim4",
            "~Yuchen_Cui1",
            "mbrown@mednet.ucla.edu"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission25/Authors"
          ]
        },
        "keywords": {
          "value": [
            "Vision-Language Models",
            "Robotic Manipulation",
            "Spatial Affordance",
            "Chain of Reasoning",
            "Interpretability",
            "Spatial Grounding",
            "Vision-Language-Action Models"
          ]
        },
        "TLDR": {
          "value": "We introduce TRACE, a new dataset and model that improves a robot's spatial understanding by training it to generate a textual 'Chain of Reasoning' before predicting an action."
        },
        "abstract": {
          "value": "Vision-Language Models (VLMs) struggle to translate high-level instructions into the precise spatial affordances required for robotic manipulation. While visual Chain-of-Thought (CoT) methods exist, they are often computationally intensive. In this work, we introduce a lightweight alternative by integrating a textual Chain of Reasoning (CoR) into the affordance prediction process. We present TRACE, a large-scale dataset created via an autonomous pipeline that pairs instructions with explicit textual rationales. By fine-tuning a VLM on this data, our model learns to externalize its spatial reasoning before acting. Our experiments show that the resulting model achieves state-of-the-art performance, with our best model reaching 55.0% accuracy on the challenging Where2Place(h) benchmark. Crucially, an ablation study demonstrates that performance scales directly with the amount of reasoning data used, confirming the CoR's effectiveness. Furthermore, analysis of the model's attention maps reveals an interpretable reasoning process where focus shifts dynamically across reasoning steps. This work shows that training VLMs to generate a textual CoR is an effective and robust strategy for enhancing the precision, reliability, and interpretability of VLM-based robot control."
        },
        "pdf": {
          "value": "/pdf/a6cb12357c4318fdfb7d29f1bc5e4132a05fdac6.pdf"
        },
        "venue": {
          "value": "KnowledgeMR"
        },
        "venueid": {
          "value": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR"
        },
        "_bibtex": {
          "value": "@inproceedings{\nanonymous2025trace,\ntitle={{TRACE}: Textual Reasoning for Affordance Coordinate Extraction},\nauthor={Anonymous},\nbooktitle={1st Workshop on Knowledge-Intensive Multimodal Reasoning},\nyear={2025},\nurl={https://openreview.net/forum?id=a3ESrzK5S6}\n}"
        },
        "paperhash": {
          "value": "park|trace_textual_reasoning_for_affordance_coordinate_extraction",
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission25/Authors"
          ]
        }
      },
      "details": {
        "replies": [
          {
            "id": "bO4YZ1yRBF",
            "forum": "a3ESrzK5S6",
            "replyto": "a3ESrzK5S6",
            "signatures": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "nonreaders": [],
            "readers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission25/Authors"
            ],
            "writers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "content": {
              "title": {
                "value": "Paper Decision"
              },
              "decision": {
                "value": "Accept"
              },
              "comment": {
                "value": "This paper presents a clear, well supported advance in language grounded robot control. It introduces TRACE, a large scale dataset that pairs instructions with explicit textual chains of reasoning and fine tunes a VLM to externalize spatial rationale before predicting affordance coordinates. The approach is lightweight compared with visual CoT, achieves state of the art accuracy on Where2Place and its hard split, and delivers sizable gains over RoboPoint, with performance scaling as reasoning data increases and attention analyses showing interpretable stepwise focus tied to actions. These results demonstrate practical impact and strong generality for manipulation.  I recommend acceptance."
              }
            },
            "number": 1,
            "invitations": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission25/-/Decision",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit"
            ],
            "domain": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "tcdate": 1756687634099,
            "cdate": 1756687634099,
            "tmdate": 1756688400139,
            "mdate": 1756688400139,
            "parentInvitations": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Decision",
            "license": "CC BY 4.0",
            "version": 2
          }
        ]
      },
      "invitations": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Post_Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission25/-/Camera_Ready"
      ],
      "readers": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission25/Authors"
      ]
    },
    {
      "id": "CAnie6bWk3",
      "number": 26,
      "forum": "CAnie6bWk3",
      "content": {
        "title": {
          "value": "RoRA-VLM: Robust Retrieval Augmentation for Vision Language Models"
        },
        "authors": {
          "value": [
            "Jingyuan Qi",
            "Zhiyang Xu",
            "Rulin Shao",
            "Zihao Lin",
            "Yang Chen",
            "Di Jin",
            "Yu Cheng",
            "Qifan Wang",
            "Lifu Huang"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission26/Authors"
          ]
        },
        "authorids": {
          "value": [
            "~Jingyuan_Qi1",
            "~Zhiyang_Xu1",
            "~Rulin_Shao1",
            "~Zihao_Lin1",
            "~Yang_Chen10",
            "~Di_Jin1",
            "~Yu_Cheng1",
            "~Qifan_Wang2",
            "~Lifu_Huang1"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission26/Authors"
          ]
        },
        "keywords": {
          "value": [
            "retrieval-augmented generation",
            "vision language model"
          ]
        },
        "TLDR": {
          "value": "Robust retrieval-augmented generation"
        },
        "abstract": {
          "value": "Recent vision-language models (VLMs), despite their broad capabilities, continue to underperform on knowledge-intensive tasks. Retrieval augmentation offers a promising solution by incorporating external multimodal knowledge. However, the retrieved content often contains a mix of relevant and irrelevant information, and existing methods primarily focus on improving retrieval quality to mitigate this issue. In this work, we propose RoRA-VLM, a robust retrieval augmentation framework designed to address the complementary challenge of utilizing noisy retrieved knowledge effectively. The core insight behind RoRA-VLM is that the multimodal nature of VLMs enables a novel solution: visual information can act as a signal for assessing the relevance of retrieved results. To this end, RoRA-VLM introduces a learned cross-modal verification mechanism that enables VLMs to compare visual similarities between the query and retrieved images, and attend selectively to visually relevant retrievals while filtering out irrelevant content. Extensive experiments on OVEN, InfoSeek, and Enc-VQA benchmarks demonstrate that RoRA-VLM achieves significant performance improvements of up to 14.76% in accuracy compared to baseline models with minimal training data, consistently outperforming state-of-the-art retrieval-augmented VLMs while exhibiting strong generalization to unseen domains."
        },
        "pdf": {
          "value": "/pdf/586512bdc8fba75f8831208e6ae69dea5b254f43.pdf"
        },
        "venue": {
          "value": "KnowledgeMR"
        },
        "venueid": {
          "value": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR"
        },
        "_bibtex": {
          "value": "@inproceedings{\nanonymous2025roravlm,\ntitle={Ro{RA}-{VLM}: Robust Retrieval Augmentation for Vision Language Models},\nauthor={Anonymous},\nbooktitle={1st Workshop on Knowledge-Intensive Multimodal Reasoning},\nyear={2025},\nurl={https://openreview.net/forum?id=CAnie6bWk3}\n}"
        },
        "paperhash": {
          "value": "qi|roravlm_robust_retrieval_augmentation_for_vision_language_models",
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission26/Authors"
          ]
        }
      },
      "details": {
        "replies": [
          {
            "id": "EepNVVkYLI",
            "forum": "CAnie6bWk3",
            "replyto": "CAnie6bWk3",
            "signatures": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "nonreaders": [],
            "readers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission26/Authors"
            ],
            "writers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "content": {
              "title": {
                "value": "Paper Decision"
              },
              "decision": {
                "value": "Accept"
              },
              "comment": {
                "value": "The paper proposes RoRA-VLM, a retrieval-augmented framework with cross-modal verification to filter noisy knowledge. Experiments on OVEN, InfoSeek, and EncVQA show substantial accuracy gains and robustness to irrelevant retrievals. The method is novel, effective, and demonstrates strong generalization. I recommend acceptance. "
              }
            },
            "number": 1,
            "invitations": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission26/-/Decision",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit"
            ],
            "domain": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "tcdate": 1756687633154,
            "cdate": 1756687633154,
            "tmdate": 1756688400140,
            "mdate": 1756688400140,
            "parentInvitations": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Decision",
            "license": "CC BY 4.0",
            "version": 2
          }
        ]
      },
      "invitations": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Post_Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit"
      ],
      "readers": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission26/Authors"
      ]
    },
    {
      "id": "eJcgcYITJi",
      "number": 27,
      "forum": "eJcgcYITJi",
      "content": {
        "title": {
          "value": "Segment Anything to Explain Anything: Concept-Level Attribution with Foundation Models under Limited Supervision"
        },
        "authors": {
          "value": [
            "Ayush Somani",
            "Rudraksh Sachin Joshi",
            "Dilip K. Prasad"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission27/Authors"
          ]
        },
        "authorids": {
          "value": [
            "~Ayush_Somani1",
            "~Rudraksh_Sachin_Joshi1",
            "~Dilip_K._Prasad1"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission27/Authors"
          ]
        },
        "keywords": {
          "value": [
            "XAI",
            "Transparency",
            "foundational model",
            "segmentation",
            "metric evaluation"
          ]
        },
        "TLDR": {
          "value": "We introduce a post-hoc framework that explains image classifiers by segmenting and naming the most influential visual concepts‚Äîmaking AI decisions more precise, interpretable, and human-aligned."
        },
        "abstract": {
          "value": "Despite the progress in visual recognition, deep vision models often lack human-aligned explanations, especially in data-sparse or safety-critical settings. We introduce Segment-Concept Explanations (SCE), a modular, post-hoc framework that leverages recent advances in segmentation and vision-language models to deliver richer concept-level, pixel-precise mask-based explanations without rigorous training. SCE begins with propagation-based saliency methods (e.g., Grad-CAM, Integrated Gradient, Occlusion) to identify discriminative regions for a model‚Äôs top-K prediction. These regions guide a lightweight vision-language model (VLM) to generate bounding boxes and coarse concept labels (e.g., head, wing, branch), which are then passed to foundational segmentation model for high-resolution segment extraction. Each segment‚Äôs influence is quantified using confidence-drop attribution ($\\Delta_{\\text{conf}}$), followed by semantic clustering and aggregation into concise, multimodal explanations. Evaluated across five datasets‚Äî CUB-200-2011, Oxford-IIIT Pet, Pascal VOC 2012, SMDG-19, and Kvasir-SEG; SCE consistently outperforms pixel-level baselines in localization (mIoU) and sensitivity (confidence/relative drop), while maintaining ``zero-label\" concept discovery and segmentation generalizability. Experiments on ResNet-50, DeiT-Base, and Swin-Tiny classifiers pretrained on ImageNet validate SCE‚Äôs broad applicability. Our framework bridges foundational models and explainability to offer scalable, human-centric lens into vision model reasoning without additional labels or retraining."
        },
        "pdf": {
          "value": "/pdf/7f5a46a01d77d6e1adda8eda311ee5dd84d028bf.pdf"
        },
        "venue": {
          "value": "KnowledgeMR"
        },
        "venueid": {
          "value": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR"
        },
        "_bibtex": {
          "value": "@inproceedings{\nanonymous2025segment,\ntitle={Segment Anything to Explain Anything: Concept-Level Attribution with Foundation Models under Limited Supervision},\nauthor={Anonymous},\nbooktitle={1st Workshop on Knowledge-Intensive Multimodal Reasoning},\nyear={2025},\nurl={https://openreview.net/forum?id=eJcgcYITJi}\n}"
        },
        "paperhash": {
          "value": "somani|segment_anything_to_explain_anything_conceptlevel_attribution_with_foundation_models_under_limited_supervision",
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission27/Authors"
          ]
        }
      },
      "details": {
        "replies": [
          {
            "id": "407NrU2QI9",
            "forum": "eJcgcYITJi",
            "replyto": "eJcgcYITJi",
            "signatures": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "nonreaders": [],
            "readers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission27/Authors"
            ],
            "writers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "content": {
              "title": {
                "value": "Paper Decision"
              },
              "decision": {
                "value": "Accept"
              },
              "comment": {
                "value": "This paper introduces Segment-Concept Explanations, a modular post hoc framework that combines saliency, a lightweight vision language model for concept proposals, and SAM2 for high resolution segmentation to produce pixel precise, concept labeled masks without retraining. Evaluations on CUB 200 2011, Oxford IIIT Pets, Pascal VOC 2012, SMDG 19, and Kvasir SEG show consistent gains in localization, stronger confidence drop attribution, and concise multimodal explanations across common backbones. The work advances explainability by moving from diffuse heatmaps to grounded parts such as head, wing, and polyp, yielding faithful and human readable evidence suited for low label and safety critical settings.  I recommend acceptance."
              }
            },
            "number": 1,
            "invitations": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission27/-/Decision",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit"
            ],
            "domain": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "tcdate": 1756687634122,
            "cdate": 1756687634122,
            "tmdate": 1756688400437,
            "mdate": 1756688400437,
            "parentInvitations": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Decision",
            "license": "CC BY 4.0",
            "version": 2
          }
        ]
      },
      "invitations": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Post_Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit"
      ],
      "readers": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission27/Authors"
      ]
    },
    {
      "id": "ogCOSyJTeP",
      "number": 28,
      "forum": "ogCOSyJTeP",
      "content": {
        "title": {
          "value": "SciVideoBench: Benchmarking Scientific Video Reasoning in Large Multimodal Models<br><i>üèÜ Best Paper Award (Benchmark Track)</i>"
        },
        "authors": {
          "value": [
            "Andong Deng",
            "Taojiannan Yang",
            "Shoubin Yu",
            "Lincoln Spencer",
            "Mohit Bansal",
            "Chen Chen",
            "Serena Yeung-Levy",
            "Xiaohan Wang"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission28/Authors"
          ]
        },
        "authorids": {
          "value": [
            "~Andong_Deng2",
            "~Taojiannan_Yang1",
            "~Shoubin_Yu1",
            "~Lincoln_Spencer1",
            "~Mohit_Bansal2",
            "~Chen_Chen18",
            "~Serena_Yeung-Levy1",
            "~Xiaohan_Wang2"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission28/Authors"
          ]
        },
        "keywords": {
          "value": [
            "AI4Science",
            "VideoLLM",
            "Video Reasoning"
          ]
        },
        "abstract": {
          "value": "Large Multimodal Models (LMMs) have achieved remarkable progress across various capabilities, however, video reasoning remains a significant and challenging frontier. Current video benchmarks predominantly target general domains with relatively simple reasoning tasks, leading to saturation and thus failing to effectively evaluate advanced multimodal cognitive skills. To address this critical gap, we introduce SciVideoBench, a rigorous benchmark specifically designed to assess advanced video reasoning in scientific contexts. \\scbench consists of 1,371 carefully crafted multiple-choice questions derived from cutting-edge experimental videos spanning over 19 specialized academic subjects. Each question demands sophisticated domain-specific knowledge, precise spatiotemporal perception, and intricate logical reasoning, effectively challenging models' higher-order cognitive abilities. \nOur evaluation highlights significant performance deficits in state-of-the-art proprietary and open-source LMMs, including Gemini 2.5 Pro and Qwen2.5-VL, indicating substantial room for advancement in video reasoning capabilities.\nDetailed analyses of critical factors such as reasoning complexity and visual grounding provide valuable insights and clear direction for future developments in LMMs, driving the evolution of truly capable multimodal AI co-scientists."
        },
        "pdf": {
          "value": "/pdf/f4174b93936c8927541eecc93c04ad28da56012b.pdf"
        },
        "venue": {
          "value": "KnowledgeMR"
        },
        "venueid": {
          "value": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR"
        },
        "_bibtex": {
          "value": "@inproceedings{\nanonymous2025scivideobench,\ntitle={SciVideoBench: Benchmarking Scientific Video Reasoning in Large Multimodal Models},\nauthor={Anonymous},\nbooktitle={1st Workshop on Knowledge-Intensive Multimodal Reasoning},\nyear={2025},\nurl={https://openreview.net/forum?id=ogCOSyJTeP}\n}"
        },
        "paperhash": {
          "value": "deng|scivideobench_benchmarking_scientific_video_reasoning_in_large_multimodal_models",
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission28/Authors"
          ]
        }
      },
      "details": {
        "replies": [
          {
            "id": "Q3hji7ivoF",
            "forum": "ogCOSyJTeP",
            "replyto": "ogCOSyJTeP",
            "signatures": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "nonreaders": [],
            "readers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission28/Authors"
            ],
            "writers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "content": {
              "title": {
                "value": "Paper Decision"
              },
              "decision": {
                "value": "Accept"
              },
              "comment": {
                "value": "This paper introduces SciVideoBench, a benchmark of 1,371 questions from scientific experimental videos across 19 disciplines. It requires domain knowledge, spatiotemporal perception, and logical reasoning. Evaluations show current LMMs perform poorly, underscoring its challenge and value. The benchmark is innovative, rigorous, and impactful for advancing scientific video reasoning. I recommend acceptance."
              }
            },
            "number": 1,
            "invitations": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission28/-/Decision",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit"
            ],
            "domain": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "tcdate": 1756687633168,
            "cdate": 1756687633168,
            "tmdate": 1756688400476,
            "mdate": 1756688400476,
            "parentInvitations": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Decision",
            "license": "CC BY 4.0",
            "version": 2
          }
        ]
      },
      "invitations": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Post_Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit"
      ],
      "readers": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission28/Authors"
      ]
    },
    {
      "id": "ogS7Z9TeGb",
      "number": 35,
      "forum": "ogS7Z9TeGb",
      "content": {
        "title": {
          "value": "FLanS: A Foundation Model for Free-Form Language-based Segmentation in Medical Images"
        },
        "authors": {
          "value": [
            "Longchao Da",
            "Rui Wang",
            "Xiaojian Xu",
            "Parminder Bhatia",
            "Taha Kass-Hout",
            "Hua Wei",
            "Cao Xiao"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission35/Authors"
          ]
        },
        "authorids": {
          "value": [
            "~Longchao_Da1",
            "~Rui_Wang11",
            "~Xiaojian_Xu1",
            "~Parminder_Bhatia1",
            "~Taha_Kass-Hout1",
            "~Hua_Wei1",
            "~Cao_Xiao2"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission35/Authors"
          ]
        },
        "keywords": {
          "value": [
            "Medical Image Segmentation",
            "Retrieval Augmented Generation",
            "Language Models"
          ]
        },
        "TLDR": {
          "value": "We propose FLanS, a medical image segmentation model that uses free-form text prompts and symmetry-aware canonicalization to achieve accurate segmentation across diverse datasets."
        },
        "abstract": {
          "value": "Medical imaging is crucial for diagnosing a patient‚Äôs health condition, and accurate segmentation of these images is essential for isolating regions of interest to ensure precise diagnosis and treatment planning. Existing methods primarily rely on bounding boxes or point-based prompts, while few have explored text-related prompts, despite clinicians often describing their observations and instructions in natural language. To address this gap, we first propose a RAG-based free-form text prompt generator that leverages the domain corpus to generate diverse and realistic descriptions. Then, we introduce FLanS, a novel medical image segmentation model that handles various free-form text prompts, including professional anatomy-informed queries, anatomy-agnostic position-driven queries, and anatomy-agnostic size-driven queries. Additionally, our model also incorporates a symmetry-aware canonicalization module to ensure consistent, accurate segmentations across varying scan orientations and reduce confusion between the anatomical position of an organ and its appearance in the scan. FLanS is trained on a large-scale dataset of over 100k medical images from 7 public datasets. Comprehensive experiments demonstrate the model‚Äôs superior language understanding and segmentation precision, along with a deep comprehension of the relationship between them, outperforming SOTA baselines on both in-domain and out-of-domain datasets."
        },
        "pdf": {
          "value": "/pdf/4d1b01a730315775b505e12f8984d9ff2542d89e.pdf"
        },
        "venue": {
          "value": "KnowledgeMR"
        },
        "venueid": {
          "value": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR"
        },
        "_bibtex": {
          "value": "@inproceedings{\nanonymous2025flans,\ntitle={{FL}anS: A Foundation Model for Free-Form Language-based Segmentation in Medical Images},\nauthor={Anonymous},\nbooktitle={1st Workshop on Knowledge-Intensive Multimodal Reasoning},\nyear={2025},\nurl={https://openreview.net/forum?id=ogS7Z9TeGb}\n}"
        },
        "paperhash": {
          "value": "da|flans_a_foundation_model_for_freeform_languagebased_segmentation_in_medical_images",
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission35/Authors"
          ]
        }
      },
      "details": {
        "replies": [
          {
            "id": "sH7TtS0des",
            "forum": "ogS7Z9TeGb",
            "replyto": "ogS7Z9TeGb",
            "signatures": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "nonreaders": [],
            "readers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission35/Authors"
            ],
            "writers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "content": {
              "title": {
                "value": "Paper Decision"
              },
              "decision": {
                "value": "Accept"
              },
              "comment": {
                "value": "this submission makes a timely and substantive contribution to language-guided medical image segmentation. \n The paper introduces FLanS, a foundation model that accepts free form text prompts across anatomy informed and anatomy agnostic settings, coupled with a symmetry aware canonicalization module that ensures orientation robust predictions. A retrieval augmented generator yields realistic clinical prompts, and the model is trained on a large multi dataset collection. Extensive experiments show consistent improvements over strong baselines on both in domain and out of domain evaluations, with clear ablations and visualizations that substantiate the design choices. I recommend acceptance."
              }
            },
            "number": 1,
            "invitations": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission35/-/Decision",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit"
            ],
            "domain": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "tcdate": 1756687634288,
            "cdate": 1756687634288,
            "tmdate": 1756688401048,
            "mdate": 1756688401048,
            "parentInvitations": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Decision",
            "license": "CC BY 4.0",
            "version": 2
          }
        ]
      },
      "invitations": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Post_Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit"
      ],
      "readers": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission35/Authors"
      ]
    },
    {
      "id": "4m6goLbPo5",
      "number": 36,
      "forum": "4m6goLbPo5",
      "content": {
        "title": {
          "value": "Chain of Functions: A Programmatic Pipeline for Fine-Grained Chart Reasoning Data Generation"
        },
        "authors": {
          "value": [
            "Zijian Li",
            "Jingjing Fu",
            "Lei Song",
            "Jiang Bian",
            "Jun Zhang",
            "Rui Wang"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission36/Authors"
          ]
        },
        "authorids": {
          "value": [
            "~Zijian_Li2",
            "~Jingjing_Fu1",
            "~Lei_Song3",
            "~Jiang_Bian1",
            "~Jun_Zhang25",
            "~Rui_Wang26"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission36/Authors"
          ]
        },
        "keywords": {
          "value": [
            "Multimodal Large Language Model",
            "Chart Understanding",
            "Reasoning data generation"
          ]
        },
        "TLDR": {
          "value": "A novel pipeline to generate reasoning data for fine-grained evaluation and reasoning enhancement for MLLMs"
        },
        "abstract": {
          "value": "Visual reasoning is crucial for multimodal large language models (MLLMs) to address complex chart queries, yet high-quality rationale data remains scarce. Existing methods leveraged (M)LLMs for data generation, but direct prompting often yields limited precision and diversity. In this paper, we propose \\textit{Chain of Functions (CoF)}, a novel programmatic reasoning data generation pipeline that utilizes freely-explored reasoning paths as supervision to ensure data precision and diversity. Specifically, it starts with human-free exploration among the atomic functions (e.g., maximum data and arithmetic operations) to generate diverse function chains, which are then translated into linguistic rationales and questions with only a moderate open-sourced LLM. \\textit{CoF} provides multiple benefits: 1) Precision: function-governed generation reduces hallucinations compared to freeform generation; 2) Diversity: enumerating function chains enables varied question taxonomies; 3) Explainability: function chains serve as built-in rationales, allowing fine-grained evaluation beyond overall accuracy; 4) Practicality: it eliminates reliance on extremely large models. Employing \\textit{CoF}, we construct the \\textit{ChartCoF} dataset, with 1.4k complex reasoning Q&A for fine-grained analysis and 50k Q&A for reasoning enhancement. Experiments show that \\textit{ChartCoF} improves performance for MLLMs on widely used benchmarks, and the fine-grained evaluation on \\textit{ChartCoF} reveals varying performance across question taxonomies and step numbers for each MLLM. Furthermore, the novel paradigm of function-governed rationale generation in \\textit{CoF} could inspire broader applications beyond charts."
        },
        "pdf": {
          "value": "/pdf/85f73010fc94b19dacaa14a6838e3dd98f7cfc1e.pdf"
        },
        "venue": {
          "value": "KnowledgeMR"
        },
        "venueid": {
          "value": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR"
        },
        "_bibtex": {
          "value": "@inproceedings{\nanonymous2025chain,\ntitle={Chain of Functions: A Programmatic Pipeline for Fine-Grained Chart Reasoning Data Generation},\nauthor={Anonymous},\nbooktitle={1st Workshop on Knowledge-Intensive Multimodal Reasoning},\nyear={2025},\nurl={https://openreview.net/forum?id=4m6goLbPo5}\n}"
        },
        "paperhash": {
          "value": "li|chain_of_functions_a_programmatic_pipeline_for_finegrained_chart_reasoning_data_generation",
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission36/Authors"
          ]
        }
      },
      "details": {
        "replies": [
          {
            "id": "TOhtlDckwa",
            "forum": "4m6goLbPo5",
            "replyto": "4m6goLbPo5",
            "signatures": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "nonreaders": [],
            "readers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission36/Authors"
            ],
            "writers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "content": {
              "title": {
                "value": "Paper Decision"
              },
              "decision": {
                "value": "Accept"
              },
              "comment": {
                "value": "This paper introduces CoF, a programmatic pipeline for chart reasoning data generation, enabling precise, diverse, and explainable supervision. The resulting ChartCoF dataset supports fine-grained evaluation across reasoning taxonomies and enhances MLLMs‚Äô performance on chart benchmarks. The approach is innovative, practical, and contributes valuable resources for visual reasoning research. I recommend acceptance. "
              }
            },
            "number": 1,
            "invitations": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission36/-/Decision",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit"
            ],
            "domain": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "tcdate": 1756687633251,
            "cdate": 1756687633251,
            "tmdate": 1756688401162,
            "mdate": 1756688401162,
            "parentInvitations": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Decision",
            "license": "CC BY 4.0",
            "version": 2
          }
        ]
      },
      "invitations": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Post_Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission36/-/Camera_Ready"
      ],
      "readers": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission36/Authors"
      ]
    },
    {
      "id": "lbeYNFJ52J",
      "number": 37,
      "forum": "lbeYNFJ52J",
      "content": {
        "title": {
          "value": "Few-Shot Vision-Language Reasoning for Satellite Imagery via Verifiable Rewards"
        },
        "authors": {
          "value": [
            "Aybora K√∂ksal",
            "A. Aydƒ±n Alatan"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission37/Authors"
          ]
        },
        "authorids": {
          "value": [
            "~Aybora_K√∂ksal1",
            "~A._Aydƒ±n_Alatan1"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission37/Authors"
          ]
        },
        "keywords": {
          "value": [
            "Vision-Language Models",
            "Reinforcement Learning",
            "Verifiable Rewards",
            "Few-Shot Learning",
            "Remote Sensing",
            "Policy Optimization",
            "Visual Question Answering",
            "Scene Classification",
            "Visual Grounding",
            "Satellite Imagery"
          ]
        },
        "TLDR": {
          "value": "Few-shot RL with verifiable rewards enables compact vision-language models to reason over satellite imagery without needing caption supervision."
        },
        "abstract": {
          "value": "Recent advances in large language and vision-language models have enabled strong reasoning capabilities, yet they remain impractical for specialized domains like remote sensing, where annotated data is scarce and expensive. We present the first few-shot reinforcement learning with verifiable reward (RLVR) framework for satellite imagery that eliminates the need for caption supervision-relying solely on lightweight, rule-based binary or IoU-based rewards. Adapting the ‚Äú1-shot RLVR‚Äù paradigm from language models to vision-language models, we employ policy-gradient optimization with as few as one curated example to align model outputs for satellite reasoning tasks. Comprehensive experiments across multiple remote sensing benchmarks-including classification, visual question answering, and grounding-show that even a single example yields substantial improvements over the base model. Scaling to 128 examples matches or exceeds models trained on thousands of annotated samples. While the extreme one-shot setting can induce mild, task-specific overfitting, our approach consistently demonstrates robust generalization and efficiency across diverse tasks. Further, we find that prompt design and loss weighting significantly influence training stability and final accuracy. Our method enables cost-effective and data-efficient development of domain-specialist vision-language reasoning models, offering a pragmatic recipe for data-scarce fields: start from a compact VLM, curate a handful of reward-checkable cases, and train via RLVR. Our model, training code and dataset will be available."
        },
        "pdf": {
          "value": "/pdf/edcc6e9d5991197b39ad06699e48eac0f75446a0.pdf"
        },
        "venue": {
          "value": "KnowledgeMR"
        },
        "venueid": {
          "value": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR"
        },
        "_bibtex": {
          "value": "@inproceedings{\nanonymous2025fewshot,\ntitle={Few-Shot Vision-Language Reasoning for Satellite Imagery via Verifiable Rewards},\nauthor={Anonymous},\nbooktitle={1st Workshop on Knowledge-Intensive Multimodal Reasoning},\nyear={2025},\nurl={https://openreview.net/forum?id=lbeYNFJ52J}\n}"
        },
        "paperhash": {
          "value": "k√∂ksal|fewshot_visionlanguage_reasoning_for_satellite_imagery_via_verifiable_rewards",
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission37/Authors"
          ]
        }
      },
      "details": {
        "replies": [
          {
            "id": "VnGVvwyteJ",
            "forum": "lbeYNFJ52J",
            "replyto": "lbeYNFJ52J",
            "signatures": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "nonreaders": [],
            "readers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission37/Authors"
            ],
            "writers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "content": {
              "title": {
                "value": "Paper Decision"
              },
              "decision": {
                "value": "Accept"
              },
              "comment": {
                "value": "This paper presents a caption-free few-shot reinforcement learning with verifiable rewards framework for satellite vision-language reasoning, adapting 1-shot RLVR to VLMs and relying only on lightweight binary and IoU rewards. It aligns a compact Qwen2-VL-2B model without caption supervision and evaluates across classification, VQA, grounding, and LHRS-Bench, showing large gains from as little as 1 to 128 shots, with 128-shot performance matching or exceeding baselines trained on thousands of annotated samples. The paper also provides actionable guidance on prompt design and KL weighting, and a practical recipe for cost-effective specialization in data-scarce domains. I recommend acceptance."
              }
            },
            "number": 1,
            "invitations": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission37/-/Decision",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit"
            ],
            "domain": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "tcdate": 1756687634324,
            "cdate": 1756687634324,
            "tmdate": 1756688401352,
            "mdate": 1756688401352,
            "parentInvitations": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Decision",
            "license": "CC BY 4.0",
            "version": 2
          }
        ]
      },
      "invitations": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Post_Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit"
      ],
      "readers": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission37/Authors"
      ]
    },
    {
      "id": "YP71w2Qk4e",
      "number": 38,
      "forum": "YP71w2Qk4e",
      "content": {
        "title": {
          "value": "MultiVerse: A Multi-Turn Conversation Benchmark for Evaluating Large Vision and Language Models"
        },
        "authors": {
          "value": [
            "Young-Jun Lee",
            "Byung-Kwan Lee",
            "Jianshu Zhang",
            "Yechan Hwang",
            "Byungsoo Ko",
            "Han-Gyu Kim",
            "Dongyu Yao",
            "Xuankun Rong",
            "Eojin Joo",
            "Seung-Ho Han",
            "Bowonko",
            "Ho-Jin Choi"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission38/Authors"
          ]
        },
        "authorids": {
          "value": [
            "~Young-Jun_Lee1",
            "~Byung-Kwan_Lee1",
            "~Jianshu_Zhang3",
            "~Yechan_Hwang1",
            "~Byungsoo_Ko1",
            "~Han-Gyu_Kim1",
            "~Dongyu_Yao1",
            "~Xuankun_Rong1",
            "~Eojin_Joo1",
            "~Seung-Ho_Han1",
            "~Bowonko1",
            "~Ho-Jin_Choi1"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission38/Authors"
          ]
        },
        "keywords": {
          "value": [
            "multi-turn conversation",
            "vision-language models"
          ]
        },
        "TLDR": {
          "value": "We introduce MultiVerse, a novel multi-turn conversation benchmark for evaluating VLMs."
        },
        "abstract": {
          "value": "Vision-and-Language Models (VLMs) have shown impressive capabilities on single-turn benchmarks, yet real-world applications often demand more intricate multi-turn dialogues. Existing multi-turn datasets (e.g, MMDU, ConvBench) only partially capture the breadth and depth of conversational scenarios encountered by users. In this work, we introduce MultiVerse, a novel multi-turn conversation benchmark featuring 647 dialogues‚Äîeach averaging four turns‚Äîderived from a diverse set of 12 popular VLM evaluation benchmarks. With 484 tasks and 484 interaction goals, MultiVerse covers a wide range of topics, from factual knowledge and perception to advanced reasoning tasks such as mathematics and coding. To facilitate robust assessment, we propose a checklist-based evaluation method that leverages GPT-4o as the automated evaluator, measuring performance across 37 key aspects, including perceptual accuracy, linguistic clarity, and factual correctness. We evaluate 18 VLMs on MultiVerse, revealing that even the strongest models (e.g., GPT-4o) achieve only a 50% success rate in complex multi-turn conversations, highlighting the dataset's challenging nature. Notably, we find that providing full dialogue context significantly enhances performance for smaller or weaker models, emphasizing the importance of in-context learning. We believe MultiVerse is a landscape of evaluating multi-turn interaction abilities for VLMs."
        },
        "pdf": {
          "value": "/pdf/0a69ea57f1bbe3d9c671c6ccf701cfce9a962258.pdf"
        },
        "venue": {
          "value": "KnowledgeMR"
        },
        "venueid": {
          "value": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR"
        },
        "_bibtex": {
          "value": "@inproceedings{\nanonymous2025multiverse,\ntitle={MultiVerse: A Multi-Turn Conversation Benchmark for Evaluating Large Vision and Language Models},\nauthor={Anonymous},\nbooktitle={1st Workshop on Knowledge-Intensive Multimodal Reasoning},\nyear={2025},\nurl={https://openreview.net/forum?id=YP71w2Qk4e}\n}"
        },
        "paperhash": {
          "value": "lee|multiverse_a_multiturn_conversation_benchmark_for_evaluating_large_vision_and_language_models",
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission38/Authors"
          ]
        }
      },
      "details": {
        "replies": [
          {
            "id": "YQtcnoBJF5",
            "forum": "YP71w2Qk4e",
            "replyto": "YP71w2Qk4e",
            "signatures": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "nonreaders": [],
            "readers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission38/Authors"
            ],
            "writers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "content": {
              "title": {
                "value": "Paper Decision"
              },
              "decision": {
                "value": "Accept"
              },
              "comment": {
                "value": "The paper presents MULTIVERSE, a comprehensive benchmark covering 647 dialogues with 484 tasks across 25 domains. It introduces checklist-based evaluation and demonstrates that even state-of-the-art VLMs struggle in multi-turn settings. The benchmark is broad, well-constructed, and highly relevant to advancing dialogue evaluation. I recommend acceptance."
              }
            },
            "number": 1,
            "invitations": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission38/-/Decision",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit"
            ],
            "domain": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "tcdate": 1756687633305,
            "cdate": 1756687633305,
            "tmdate": 1756688401627,
            "mdate": 1756688401627,
            "parentInvitations": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Decision",
            "license": "CC BY 4.0",
            "version": 2
          }
        ]
      },
      "invitations": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Post_Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission38/-/Camera_Ready"
      ],
      "readers": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission38/Authors"
      ]
    },
    {
      "id": "ixo0i897ZR",
      "number": 39,
      "forum": "ixo0i897ZR",
      "content": {
        "title": {
          "value": "RealDPO: Real or Not Real, that is the Preference"
        },
        "authors": {
          "value": [
            "Danni Yang",
            "Ziqi Huang",
            "Jianlou Si",
            "Ziwei Liu"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission39/Authors"
          ]
        },
        "authorids": {
          "value": [
            "~Danni_Yang1",
            "~Ziqi_Huang2",
            "~Jianlou_Si1",
            "~Ziwei_Liu1"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission39/Authors"
          ]
        },
        "keywords": {
          "value": [
            "Image to Video generation",
            "Direct Preference Optimization"
          ]
        },
        "TLDR": {
          "value": "We proposed RealDPO to employ Direct Preference Optimization to enhance video generation."
        },
        "abstract": {
          "value": "Video generative models have recently achieved notable advancements in synthesis quality. However, generating complex motions remains a critical challenge, as existing models often struggle to produce natural, smooth, and contextually consistent movements. This gap between generated and real-world motions limits their practical applicability. To address this issue, we introduce RealDPO, a novel alignment paradigm that leverages real-world data as positive samples for preference learning, enabling more accurate motion synthesis. Unlike traditional supervised fine-tuning (SFT), which offers limited corrective feedback, RealDPO employs Direct Preference Optimization (DPO) with a tailored loss function to enhance motion realism. By contrasting real-world videos with erroneous model outputs, RealDPO enables iterative self-correction, progressively refining motion quality. To support post-training in complex motion synthesis, we propose RealAction-5K, a curated dataset of high-quality videos capturing human daily activities with rich and precise motion details. Extensive experiments demonstrate that RealDPO significantly improves video quality, text alignment, and motion realism compared to state-of-the-art models and existing preference optimization techniques."
        },
        "pdf": {
          "value": "/pdf/301cbc1ab51b2143b998d7567d7e59bdd7314335.pdf"
        },
        "venue": {
          "value": "KnowledgeMR"
        },
        "venueid": {
          "value": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR"
        },
        "_bibtex": {
          "value": "@inproceedings{\nanonymous2025realdpo,\ntitle={Real{DPO}: Real or Not Real, that is the Preference},\nauthor={Anonymous},\nbooktitle={1st Workshop on Knowledge-Intensive Multimodal Reasoning},\nyear={2025},\nurl={https://openreview.net/forum?id=ixo0i897ZR}\n}"
        },
        "paperhash": {
          "value": "yang|realdpo_real_or_not_real_that_is_the_preference",
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission39/Authors"
          ]
        }
      },
      "details": {
        "replies": [
          {
            "id": "JIRsqxt3G7",
            "forum": "ixo0i897ZR",
            "replyto": "ixo0i897ZR",
            "signatures": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "nonreaders": [],
            "readers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission39/Authors"
            ],
            "writers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "content": {
              "title": {
                "value": "Paper Decision"
              },
              "decision": {
                "value": "Accept"
              },
              "comment": {
                "value": "This submission presents RealDPO, a data-efficient alignment framework for action-centric video generation. The core idea is to use real videos as positive preference samples in Direct Preference Optimization, removing dependence on reward models while improving motion realism. The authors design a DPO loss tailored to diffusion transformers and introduce RealAction-5K, a curated dataset of daily human actions. Extensive experiments on CogVideoX-5B show consistent gains in visual and text alignment and human motion quality, corroborated by user studies, MLLM-based evaluation, and VBench metrics, with qualitative examples surpassing SFT and reward-based baselines. I recommend acceptance"
              }
            },
            "number": 1,
            "invitations": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission39/-/Decision",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit"
            ],
            "domain": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "tcdate": 1756687634441,
            "cdate": 1756687634441,
            "tmdate": 1756688401733,
            "mdate": 1756688401733,
            "parentInvitations": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Decision",
            "license": "CC BY 4.0",
            "version": 2
          }
        ]
      },
      "invitations": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Post_Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit"
      ],
      "readers": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission39/Authors"
      ]
    },
    {
      "id": "bAFjHrJSUc",
      "number": 40,
      "forum": "bAFjHrJSUc",
      "content": {
        "title": {
          "value": "VChain: Chain-of-Visual-Thought for Reasoning in Video Generation<br><i>üèÜ Outstanding Paper Award (Main Track)</i>"
        },
        "authors": {
          "value": [
            "Ziqi Huang",
            "Ning Yu",
            "Gordon Chen",
            "Haonan Qiu",
            "Paul Debevec",
            "Ziwei Liu"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission40/Authors"
          ]
        },
        "authorids": {
          "value": [
            "~Ziqi_Huang2",
            "~Ning_Yu2",
            "~Gordon_Chen1",
            "~Haonan_Qiu1",
            "~Paul_Debevec1",
            "~Ziwei_Liu1"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission40/Authors"
          ]
        },
        "keywords": {
          "value": [
            "video generation",
            "multimodal reasoning"
          ]
        },
        "TLDR": {
          "value": "We introduce VChain, an inference-time scaling framework for reasoning in video generation."
        },
        "abstract": {
          "value": "Recent video generation models can produce smooth and visually appealing clips, but they often struggle to synthesize complex dynamics with a coherent chain of consequences. Accurately modeling visual outcomes and state transitions over time remains a core challenge. In contrast, large language and multimodal models (e.g., GPT-4o) exhibit strong visual state reasoning and future prediction capabilities. To bridge these strengths, we introduce VChain, a novel inference-time chain-of-visual-thought framework that injects visual reasoning signals from multimodal models into video generation. Specifically, VChain contains a dedicated pipeline that leverages large multimodal models to generate a sparse set of critical keyframes as snapshots, which are then used to guide the sparse inference-time tuning of a pre-trained video generator only at these key moments. Our approach is tuning-efficient, introduces minimal overhead and avoids dense supervision. Extensive experiments on complex, multi-step scenarios show that VChain significantly enhances the quality of generated videos."
        },
        "pdf": {
          "value": "/pdf/826bcd77e83199aa127af37a26341c6e80465428.pdf"
        },
        "venue": {
          "value": "KnowledgeMR"
        },
        "venueid": {
          "value": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR"
        },
        "_bibtex": {
          "value": "@inproceedings{\nanonymous2025vchain,\ntitle={{VC}hain: Chain-of-Visual-Thought for Reasoning in Video Generation},\nauthor={Anonymous},\nbooktitle={1st Workshop on Knowledge-Intensive Multimodal Reasoning},\nyear={2025},\nurl={https://openreview.net/forum?id=bAFjHrJSUc}\n}"
        },
        "paperhash": {
          "value": "huang|vchain_chainofvisualthought_for_reasoning_in_video_generation",
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission40/Authors"
          ]
        }
      },
      "details": {
        "replies": [
          {
            "id": "QzxfgeAeeu",
            "forum": "bAFjHrJSUc",
            "replyto": "bAFjHrJSUc",
            "signatures": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "nonreaders": [],
            "readers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission40/Authors"
            ],
            "writers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "content": {
              "title": {
                "value": "Paper Decision"
              },
              "decision": {
                "value": "Accept"
              },
              "comment": {
                "value": "This paper introduces VChain, an inference-time chain-of-visual-thought framework that injects reasoning into video generation. The method uses a multimodal model to derive sparse, causally important keyframes and paired textual thoughts, then performs lightweight LoRA tuning of a pretrained generator only at those moments. This design is self-contained, efficient, and plug-and-play, and it demonstrably improves physics, commonsense, and causal coherence while maintaining or slightly improving standard quality metrics. The experiments combine quantitative benchmarks and human judgments, with clear ablations that isolate the value of visual thoughts and sparse tuning. The contribution is timely, novel, and impactful.  I recommend acceptance."
              }
            },
            "number": 1,
            "invitations": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission40/-/Decision",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit"
            ],
            "domain": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "tcdate": 1756687633309,
            "cdate": 1756687633309,
            "tmdate": 1756688401868,
            "mdate": 1756688401868,
            "parentInvitations": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Decision",
            "license": "CC BY 4.0",
            "version": 2
          }
        ]
      },
      "invitations": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Post_Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit"
      ],
      "readers": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission40/Authors"
      ]
    },
    {
      "id": "rvqjZTgS5U",
      "number": 41,
      "forum": "rvqjZTgS5U",
      "content": {
        "title": {
          "value": "CAMA: Enhancing Multimodal In-Context Learning with Context-Aware Modulated Attention"
        },
        "authors": {
          "value": [
            "Yanshu Li",
            "Jiangjiang Yang",
            "Ziteng Yang",
            "Bozheng Li",
            "Hongyang He",
            "Ligong Han",
            "Yingjie Victor Chen",
            "Songlin Fei",
            "Dongfang Liu",
            "Ruixiang Tang"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission41/Authors"
          ]
        },
        "authorids": {
          "value": [
            "~Yanshu_Li1",
            "~Jiangjiang_Yang1",
            "~Ziteng_Yang1",
            "~Bozheng_Li1",
            "~Hongyang_He1",
            "~Ligong_Han1",
            "~Yingjie_Victor_Chen1",
            "~Songlin_Fei1",
            "~Dongfang_Liu1",
            "~Ruixiang_Tang1"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission41/Authors"
          ]
        },
        "keywords": {
          "value": [
            "Large Vision-language Model",
            "In-context Learning",
            "Attention Dynamics"
          ]
        },
        "abstract": {
          "value": "Multimodal in-context learning (ICL) is emerging as a key capability that enables large vision-language models (LVLMs) to adapt to novel tasks without parameter updates, expanding their utility across various real-world applications. However, ICL remains unstable, even with well-matched in-context demonstrations (ICDs), suggesting that LVLMs struggle to fully utilize the provided context. While existing efforts focus on prompt engineering or post‚Äëhoc logit calibration, we instead investigate the underlying attention dynamics to overcome LVLMs' inherent limitations. We identify two critical deficits in their self-attention that impair effective ICL. To bridge the gap, we propose \\textbf{Context-Aware Modulated Attention} (CAMA), a plug-and-play and training-free method that dynamically modulates LVLM's attention logits based on the input in‚Äëcontext sequence. CAMA employs a two-stage attention modulation to address both identified deficits, enhancing the focus on semantically significant tokens, particularly visual ones. Across four LVLMs and seven benchmarks, CAMA consistently outperforms vanilla models and baselines, demonstrating great effectiveness and generalization. It can also activate the desired effects of prompt engineering methods and remains robust under diverse sequence configurations. Thus, CAMA paves the way for deeper explorations of attention dynamics to advance multimodal reasoning."
        },
        "pdf": {
          "value": "/pdf/c7a3298e12520f3694d11f567e486de9d023e479.pdf"
        },
        "venue": {
          "value": "KnowledgeMR"
        },
        "venueid": {
          "value": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR"
        },
        "_bibtex": {
          "value": "@inproceedings{\nanonymous2025cama,\ntitle={{CAMA}: Enhancing Multimodal In-Context Learning with Context-Aware Modulated Attention},\nauthor={Anonymous},\nbooktitle={1st Workshop on Knowledge-Intensive Multimodal Reasoning},\nyear={2025},\nurl={https://openreview.net/forum?id=rvqjZTgS5U}\n}"
        },
        "paperhash": {
          "value": "li|cama_enhancing_multimodal_incontext_learning_with_contextaware_modulated_attention",
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission41/Authors"
          ]
        }
      },
      "details": {
        "replies": [
          {
            "id": "qhHX20TJ7O",
            "forum": "rvqjZTgS5U",
            "replyto": "rvqjZTgS5U",
            "signatures": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "nonreaders": [],
            "readers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission41/Authors"
            ],
            "writers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "content": {
              "title": {
                "value": "Paper Decision"
              },
              "decision": {
                "value": "Accept"
              },
              "comment": {
                "value": "This paper addresses instability in multimodal in-context learning by diagnosing attention deficits in LVLMs and introducing Context-Aware Modulated Attention (CAMA), a plug-and-play, training-free modulation of attention logits. CAMA applies two stages: intra-ICD grounding in shallow layers to emphasize text-aligned visual tokens, and query-centric routing in middle layers to prioritize relevant demonstrations. Across four LVLMs and seven VQA benchmarks, plus captioning, classification, and storytelling, CAMA consistently outperforms vanilla models and baselines, amplifies the benefits of prompt methods, and remains robust across shots and configurations, indicating strong generalization and practical value. The analysis offers mechanistic insight with actionable improvements. I recommend acceptance."
              }
            },
            "number": 1,
            "invitations": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission41/-/Decision",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit"
            ],
            "domain": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "tcdate": 1756687634449,
            "cdate": 1756687634449,
            "tmdate": 1756688401967,
            "mdate": 1756688401967,
            "parentInvitations": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Decision",
            "license": "CC BY 4.0",
            "version": 2
          }
        ]
      },
      "invitations": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Post_Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit"
      ],
      "readers": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission41/Authors"
      ]
    },
    {
      "id": "fUmbrYIJ7T",
      "number": 43,
      "forum": "fUmbrYIJ7T",
      "content": {
        "title": {
          "value": "Machine Mental Imagery: Empower Multimodal Reasoning with Latent Visual Tokens<br><i>üèÜ Best Paper Award (Main Track)</i>"
        },
        "authors": {
          "value": [
            "Zeyuan Yang",
            "Xueyang Yu",
            "Delin Chen",
            "Maohao Shen",
            "Chuang Gan"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission43/Authors"
          ]
        },
        "authorids": {
          "value": [
            "~Zeyuan_Yang3",
            "~Xueyang_Yu1",
            "~Delin_Chen1",
            "~Maohao_Shen1",
            "~Chuang_Gan1"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission43/Authors"
          ]
        },
        "keywords": {
          "value": [
            "VLMs",
            "Latent Reasoning",
            "Spatial Intelligence"
          ]
        },
        "abstract": {
          "value": "Vision-language models (VLMs) excel at multimodal understanding, yet their text-only decoding forces them to verbalize visual reasoning, limiting performance on tasks that demand visual imagination. Recent attempts train VLMs to render explicit images, but the heavy image-generation pre-training often hinders the reasoning ability. Inspired by the way humans reason with mental imagery‚Äîthe internal construction and manipulation of visual cues‚Äîwe investigate whether VLMs can reason through interleaved multimodal trajectories without producing explicit images. To this end, we present a Machine Mental Imagery framework, dubbed as **Mirage**, which augments VLM decoding with latent visual tokens alongside ordinary text. Concretely, whenever the model chooses to \"think visually\", it recasts its hidden states as next tokens, thereby continuing a multimodal trajectory without generating pixel-level images. Begin by supervising the latent tokens through distillation from ground-truth image embeddings, we then switch to text-only supervision to make the latent trajectory align tightly with the task objective. A subsequent reinforcement learning stage further enhances the multimodal reasoning capability. Experiments on diverse benchmarks demonstrate that Mirage unlocks stronger multimodal reasoning without explicit image generation."
        },
        "pdf": {
          "value": "/pdf/1bad8f05faf26ff1c7bc0215a48b5554996a5133.pdf"
        },
        "venue": {
          "value": "KnowledgeMR"
        },
        "venueid": {
          "value": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR"
        },
        "_bibtex": {
          "value": "@inproceedings{\nanonymous2025machine,\ntitle={Machine Mental Imagery: Empower Multimodal Reasoning with Latent Visual Tokens},\nauthor={Anonymous},\nbooktitle={1st Workshop on Knowledge-Intensive Multimodal Reasoning},\nyear={2025},\nurl={https://openreview.net/forum?id=fUmbrYIJ7T}\n}"
        },
        "paperhash": {
          "value": "yang|machine_mental_imagery_empower_multimodal_reasoning_with_latent_visual_tokens",
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission43/Authors"
          ]
        }
      },
      "details": {
        "replies": [
          {
            "id": "zFh0CVveAt",
            "forum": "fUmbrYIJ7T",
            "replyto": "fUmbrYIJ7T",
            "signatures": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "nonreaders": [],
            "readers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission43/Authors"
            ],
            "writers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "content": {
              "title": {
                "value": "Paper Decision"
              },
              "decision": {
                "value": "Accept"
              },
              "comment": {
                "value": "This paper makes a timely and well-executed contribution: Mirage, a machine mental imagery framework that augments VLM decoding with latent visual tokens interleaved with text, enabling multimodal reasoning without explicit image generation. The authors present a clear two-stage training pipeline that grounds and then relaxes latent tokens, followed by reinforcement learning, and they show consistent gains on VSP, Jigsaw, SAT, and COMT over text-only baselines and unified generation models. Ablations and analyses indicate the latent tokens encode meaningful visual cues that improve spatial reasoning, and the writing is clear and persuasive. I recommend acceptance."
              }
            },
            "number": 1,
            "invitations": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission43/-/Decision",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit"
            ],
            "domain": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "tcdate": 1756687634465,
            "cdate": 1756687634465,
            "tmdate": 1756688402282,
            "mdate": 1756688402282,
            "parentInvitations": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Decision",
            "license": "CC BY 4.0",
            "version": 2
          }
        ]
      },
      "invitations": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Post_Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit"
      ],
      "readers": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission43/Authors"
      ]
    },
    {
      "id": "LHNQc9dh6c",
      "number": 44,
      "forum": "LHNQc9dh6c",
      "content": {
        "title": {
          "value": "Disassociating Reasoning From Knowledge in Visual Language Models with Anti-Common-Sense Premise"
        },
        "authors": {
          "value": [
            "Bingyang Wang",
            "Yijiang Li",
            "Jiechen Song",
            "Xuzhe Hou",
            "Tianwei Zhao",
            "Chenyue Li",
            "Yizirui Fang",
            "Nuno Vasconcelos"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission44/Authors"
          ]
        },
        "authorids": {
          "value": [
            "~Bingyang_Wang2",
            "~Yijiang_Li1",
            "~Jiechen_Song1",
            "~Xuzhe_Hou1",
            "~Tianwei_Zhao1",
            "~Chenyue_Li4",
            "~Yizirui_Fang1",
            "~Nuno_Vasconcelos1"
          ],
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission44/Authors"
          ]
        },
        "keywords": {
          "value": [
            "Multimodal Reasoning",
            "Visual Entailment",
            "Vision-Language Models (VLMs)",
            "Dataset and Benchmark"
          ]
        },
        "abstract": {
          "value": "A challenge in multimodal learning is disentangling a model‚Äôs ability to perform premise-based logical reasoning from its reliance on background knowledge or co-occurrence heuristics. Existing visual entailment datasets often blur this boundary, allowing models to succeed through object recognition or plausibility judgments rather than controlled reasoning. To address this, we introduce VisReasonBench, a multimodal benchmark of 2,643 carefully constructed image-text pairs designed to probe reasoning under counterfactual, anti-commonsense, and logically perturbed premises. Examples form triplets of entailment, neutral, and contradiction choices, naturally requiring models to distinguish logical entailment from mere plausibility. We evaluate over 80 vision-language models, including CNNs, CLIP variants, Stable Diffusion, and multimodal large language models (MLLMs). Across architectures and scales, we observe that while CLIP models demonstrate some resilience to knowledge bias through contrastive grounding, MLLMs and ResNets with a classification head show pronounced susceptibility to knowledge priors over logical inference. By providing a fully human-reviewed, shortcut-resistant dataset, VisReasonBench enables precise error analysis, exposes reasoning gaps across model families, and serves as a foundation for developing training strategies that emphasize controlled logical inference and robustness against hallucination."
        },
        "pdf": {
          "value": "/pdf/ffc8231fe79055b61bf846ff5ae6873c2492ad13.pdf"
        },
        "venue": {
          "value": "KnowledgeMR"
        },
        "venueid": {
          "value": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR"
        },
        "_bibtex": {
          "value": "@inproceedings{\nanonymous2025disassociating,\ntitle={Disassociating Reasoning From Knowledge in Visual Language Models with Anti-Common-Sense Premise},\nauthor={Anonymous},\nbooktitle={1st Workshop on Knowledge-Intensive Multimodal Reasoning},\nyear={2025},\nurl={https://openreview.net/forum?id=LHNQc9dh6c}\n}"
        },
        "paperhash": {
          "value": "wang|disassociating_reasoning_from_knowledge_in_visual_language_models_with_anticommonsense_premise",
          "readers": [
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission44/Authors"
          ]
        }
      },
      "details": {
        "replies": [
          {
            "id": "0NT0CpcgM1",
            "forum": "LHNQc9dh6c",
            "replyto": "LHNQc9dh6c",
            "signatures": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "nonreaders": [],
            "readers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission44/Authors"
            ],
            "writers": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Program_Chairs"
            ],
            "content": {
              "title": {
                "value": "Paper Decision"
              },
              "decision": {
                "value": "Accept"
              },
              "comment": {
                "value": "The paper introduces VisReasonBench, a carefully curated benchmark of 2,643 NLI-style multimodal examples designed to separate logical reasoning from knowledge bias. Evaluation across 80+ models reveals systematic reliance on plausibility over logic. This benchmark fills a critical gap and offers actionable insights for robust multimodal reasoning. I recommend acceptance."
              }
            },
            "number": 1,
            "invitations": [
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission44/-/Decision",
              "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit"
            ],
            "domain": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
            "tcdate": 1756687633452,
            "cdate": 1756687633452,
            "tmdate": 1756688402728,
            "mdate": 1756688402728,
            "parentInvitations": "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Decision",
            "license": "CC BY 4.0",
            "version": 2
          }
        ]
      },
      "invitations": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Post_Submission",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/-/Edit"
      ],
      "readers": [
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR",
        "thecvf.com/ICCV/2025/Workshop/KnowledgeMR/Submission44/Authors"
      ]
    }
  ],
  "count": 42,
  "fromCache": true
}